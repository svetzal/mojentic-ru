<!DOCTYPE HTML>
<html lang="en" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Mojentic (Rust)</title>
        <meta name="robots" content="noindex">


        <!-- Custom HTML head -->

        <meta name="description" content="Agentic framework in Rust: asynchronous pubsub architecture for orchestrating LLM-powered agents.">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="highlight-css" href="highlight.css">
        <link rel="stylesheet" id="tomorrow-night-css" href="tomorrow-night.css">
        <link rel="stylesheet" id="ayu-highlight-css" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="theme/custom.css">


        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "";
            const default_light_theme = "light";
            const default_dark_theme = "navy";
            window.path_to_searchindex_js = "searchindex.js";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="toc.js"></script>
    </head>
    <body>
    <div id="mdbook-help-container">
        <div id="mdbook-help-popup">
            <h2 class="mdbook-help-title">Keyboard shortcuts</h2>
            <div>
                <p>Press <kbd>←</kbd> or <kbd>→</kbd> to navigate between chapters</p>
                <p>Press <kbd>S</kbd> or <kbd>/</kbd> to search in the book</p>
                <p>Press <kbd>?</kbd> to show this help</p>
                <p>Press <kbd>Esc</kbd> to hide this help</p>
            </div>
        </div>
    </div>
    <div id="body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            let sidebar = null;
            const sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
                sidebar_toggle.checked = false;
            }
            if (sidebar === 'visible') {
                sidebar_toggle.checked = true;
            } else {
                html.classList.remove('sidebar-visible');
            }
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="toc.html"></iframe>
            </noscript>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="default_theme">Auto</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search (`/`)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="/ s" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Mojentic (Rust)</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <div class="search-wrapper">
                            <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                            <div class="spinner-wrapper">
                                <i class="fa fa-spinner fa-spin"></i>
                            </div>
                        </div>
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="mojentic-for-rust"><a class="header" href="#mojentic-for-rust">Mojentic for Rust</a></h1>
<p>Mojentic is an agentic framework with an asynchronous pub/sub architecture designed to orchestrate LLM-powered agents. This Rust crate focuses on high performance, strong typing, and reliability.</p>
<ul>
<li>Fast, async-first event processing</li>
<li>Agents compose into complex systems</li>
<li>Clear separation between Core (LLM primitives) and Agents (behaviors)</li>
</ul>
<p>If you’re familiar with the Python project, this book mirrors its structure and concepts, adapted to Rust idioms.</p>
<h2 id="quick-links"><a class="header" href="#quick-links">Quick links</a></h2>
<ul>
<li>Getting Started → <a href="get_started.html">Get set up</a></li>
<li>LLM usage → <a href="broker.html">Broker and gateways</a></li>
<li>Core concepts → <a href="core/README.html">Layer 1</a></li>
<li>Agents → <a href="agents/README.html">Layer 2</a></li>
<li>Observability → <a href="observability/tracer.html">Tracer</a></li>
<li>API Docs → <a href="api.html">Rust API reference</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="getting-started"><a class="header" href="#getting-started">Getting Started</a></h1>
<p>This section helps you install and begin using Mojentic in Rust.</p>
<h2 id="install"><a class="header" href="#install">Install</a></h2>
<p>Add the dependency (placeholder until published):</p>
<pre><code class="language-toml">[dependencies]
mojentic = { path = "../" }
</code></pre>
<h2 id="minimal-example"><a class="header" href="#minimal-example">Minimal Example</a></h2>
<pre><pre class="playground"><code class="language-rust">use mojentic::{Broker, Message};

fn main() {
    let mut broker = Broker::default();
    broker.emit(Message::text("hello world"));
}</code></pre></pre>
<h2 id="next-steps"><a class="header" href="#next-steps">Next Steps</a></h2>
<ul>
<li>Learn the LLM broker → <a href="broker.html">Broker</a></li>
<li>Dive into Core concepts → <a href="core/README.html">Layer 1 overview</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="using-llms-broker"><a class="header" href="#using-llms-broker">Using LLMs (Broker)</a></h1>
<p>Mojentic’s LLM broker routes completion requests to pluggable gateways (e.g., Ollama). It provides a unified API for chat and text completions.</p>
<h2 id="quick-chat-example"><a class="header" href="#quick-chat-example">Quick chat example</a></h2>
<pre><pre class="playground"><code class="language-rust">use mojentic::llm::{Broker, CompletionConfig, Message};

#[tokio::main]
async fn main() -&gt; anyhow::Result&lt;()&gt; {
    let broker = Broker::new()?;
    let cfg = CompletionConfig::default();

    let resp = broker.chat(
        cfg,
        [
            Message::system("You are a helpful assistant"),
            Message::user("Say hi in one sentence"),
        ],
    ).await?;

    println!("{}", resp.text());
    Ok(())
}</code></pre></pre>
<h2 id="gateways"><a class="header" href="#gateways">Gateways</a></h2>
<ul>
<li>Ollama: local models for fast iteration.</li>
<li>HTTP-based gateways: add your own by implementing the <code>Gateway</code> trait.</li>
</ul>
<h2 id="structured-output"><a class="header" href="#structured-output">Structured output</a></h2>
<p>Use typed schemas to parse the model output into structs. See <a href="core/structured_output.html">Structured Output</a>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="quick-reference"><a class="header" href="#quick-reference">Quick Reference</a></h1>
<h2 id="installation"><a class="header" href="#installation">Installation</a></h2>
<pre><code class="language-toml">[dependencies]
mojentic = { path = "./mojentic-rust" }
tokio = { version = "1", features = ["full"] }
serde = { version = "1", features = ["derive"] }
schemars = "0.8"  # For structured output
</code></pre>
<h2 id="basic-usage"><a class="header" href="#basic-usage">Basic Usage</a></h2>
<h3 id="import-the-prelude"><a class="header" href="#import-the-prelude">Import the prelude</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use mojentic::prelude::*;
use std::sync::Arc;
<span class="boring">}</span></code></pre></pre>
<h3 id="create-a-broker"><a class="header" href="#create-a-broker">Create a broker</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let gateway = OllamaGateway::new();
let broker = LlmBroker::new("qwen3:32b", Arc::new(gateway));
<span class="boring">}</span></code></pre></pre>
<h3 id="simple-text-generation"><a class="header" href="#simple-text-generation">Simple text generation</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let messages = vec![LlmMessage::user("Hello, how are you?")];
let response = broker.generate(&amp;messages, None, None).await?;
<span class="boring">}</span></code></pre></pre>
<h2 id="message-types"><a class="header" href="#message-types">Message Types</a></h2>
<h3 id="user-message"><a class="header" href="#user-message">User message</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>LlmMessage::user("Your message here")
<span class="boring">}</span></code></pre></pre>
<h3 id="system-message"><a class="header" href="#system-message">System message</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>LlmMessage::system("You are a helpful assistant")
<span class="boring">}</span></code></pre></pre>
<h3 id="assistant-message"><a class="header" href="#assistant-message">Assistant message</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>LlmMessage::assistant("Response from assistant")
<span class="boring">}</span></code></pre></pre>
<h3 id="message-with-images"><a class="header" href="#message-with-images">Message with images</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>LlmMessage::user("Describe this image")
    .with_images(vec!["path/to/image.jpg".to_string()])
<span class="boring">}</span></code></pre></pre>
<h2 id="structured-output-1"><a class="header" href="#structured-output-1">Structured Output</a></h2>
<h3 id="define-your-type"><a class="header" href="#define-your-type">Define your type</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use serde::{Deserialize, Serialize};
use schemars::JsonSchema;

#[derive(Debug, Serialize, Deserialize, JsonSchema)]
struct Person {
    name: String,
    age: u32,
    occupation: String,
}
<span class="boring">}</span></code></pre></pre>
<h3 id="generate-typed-response"><a class="header" href="#generate-typed-response">Generate typed response</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let messages = vec![
    LlmMessage::user("Extract person info: John Doe, 30, software engineer")
];

let person: Person = broker.generate_object(&amp;messages, None).await?;
<span class="boring">}</span></code></pre></pre>
<h2 id="configuration"><a class="header" href="#configuration">Configuration</a></h2>
<h3 id="default-config"><a class="header" href="#default-config">Default config</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let config = CompletionConfig::default();
// temperature: 1.0
// num_ctx: 32768
// max_tokens: 16384
// num_predict: None
// top_p: None
// top_k: None
// response_format: None
<span class="boring">}</span></code></pre></pre>
<h3 id="custom-config"><a class="header" href="#custom-config">Custom config</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let config = CompletionConfig {
    temperature: 0.7,
    num_ctx: 16384,
    max_tokens: 8192,
    num_predict: Some(1000),
    top_p: Some(0.9),
    top_k: Some(40),
    response_format: None,
};

let response = broker.generate(&amp;messages, None, Some(config)).await?;
<span class="boring">}</span></code></pre></pre>
<h3 id="json-response-format"><a class="header" href="#json-response-format">JSON response format</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use mojentic::llm::gateway::ResponseFormat;

// Request JSON without schema
let config = CompletionConfig {
    temperature: 0.7,
    num_ctx: 16384,
    max_tokens: 8192,
    num_predict: None,
    top_p: None,
    top_k: None,
    response_format: Some(ResponseFormat::JsonObject { schema: None }),
};

// Request JSON with schema
let schema = serde_json::json!({
    "type": "object",
    "properties": {
        "name": {"type": "string"},
        "age": {"type": "number"}
    }
});

let config = CompletionConfig {
    temperature: 0.7,
    num_ctx: 16384,
    max_tokens: 8192,
    num_predict: None,
    top_p: None,
    top_k: None,
    response_format: Some(ResponseFormat::JsonObject {
        schema: Some(schema)
    }),
};
<span class="boring">}</span></code></pre></pre>
<h2 id="gateway-configuration"><a class="header" href="#gateway-configuration">Gateway Configuration</a></h2>
<h3 id="ollama-with-custom-host"><a class="header" href="#ollama-with-custom-host">Ollama with custom host</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let gateway = OllamaGateway::with_host("http://localhost:11434");
<span class="boring">}</span></code></pre></pre>
<h3 id="ollama-with-environment-variable"><a class="header" href="#ollama-with-environment-variable">Ollama with environment variable</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Set OLLAMA_HOST environment variable
std::env::set_var("OLLAMA_HOST", "http://custom-host:11434");
let gateway = OllamaGateway::new();
<span class="boring">}</span></code></pre></pre>
<h2 id="error-handling"><a class="header" href="#error-handling">Error Handling</a></h2>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use mojentic::Result;

async fn my_function() -&gt; Result&lt;String&gt; {
    let gateway = OllamaGateway::new();
    let broker = LlmBroker::new("qwen3:32b", Arc::new(gateway));

    let messages = vec![LlmMessage::user("Hello")];

    match broker.generate(&amp;messages, None, None).await {
        Ok(response) =&gt; Ok(response),
        Err(e) =&gt; {
            eprintln!("Error: {}", e);
            Err(e)
        }
    }
}
<span class="boring">}</span></code></pre></pre>
<h2 id="common-patterns"><a class="header" href="#common-patterns">Common Patterns</a></h2>
<h3 id="multi-turn-conversation"><a class="header" href="#multi-turn-conversation">Multi-turn conversation</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let mut messages = vec![
    LlmMessage::system("You are a helpful assistant"),
];

// First turn
messages.push(LlmMessage::user("What is Rust?"));
let response1 = broker.generate(&amp;messages, None, None).await?;
messages.push(LlmMessage::assistant(response1));

// Second turn
messages.push(LlmMessage::user("What are its benefits?"));
let response2 = broker.generate(&amp;messages, None, None).await?;
<span class="boring">}</span></code></pre></pre>
<h3 id="async-parallel-requests"><a class="header" href="#async-parallel-requests">Async parallel requests</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use tokio::try_join;

let broker = Arc::new(broker);
let broker1 = broker.clone();
let broker2 = broker.clone();

let (response1, response2) = try_join!(
    async move {
        broker1.generate(
            &amp;[LlmMessage::user("Question 1")],
            None,
            None
        ).await
    },
    async move {
        broker2.generate(
            &amp;[LlmMessage::user("Question 2")],
            None,
            None
        ).await
    }
)?;
<span class="boring">}</span></code></pre></pre>
<h3 id="model-management-ollama"><a class="header" href="#model-management-ollama">Model management (Ollama)</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let gateway = OllamaGateway::new();

// List available models
let models = gateway.get_available_models().await?;

// Pull a model
gateway.pull_model("qwen3:32b").await?;

// Calculate embeddings
let embedding = gateway.calculate_embeddings(
    "Some text to embed",
    Some("mxbai-embed-large")
).await?;
<span class="boring">}</span></code></pre></pre>
<h2 id="logging"><a class="header" href="#logging">Logging</a></h2>
<p>Enable logging with tracing:</p>
<pre><pre class="playground"><code class="language-rust">// In main.rs
use tracing_subscriber;

#[tokio::main]
async fn main() {
    // Initialize logging
    tracing_subscriber::fmt::init();

    // Your code here
}</code></pre></pre>
<p>Set log level via environment variable:</p>
<pre><code class="language-bash">RUST_LOG=debug cargo run
RUST_LOG=info cargo run
RUST_LOG=mojentic=debug cargo run
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="layer-1---core"><a class="header" href="#layer-1---core">Layer 1 - Core</a></h1>
<p>Layer 1 provides primitives for interacting with LLMs and composing messages, tools, and structured outputs.</p>
<h2 id="components"><a class="header" href="#components">Components</a></h2>
<ul>
<li>Messages: typed containers for role + content.</li>
<li>Completion Config: tuning parameters (temperature, max tokens, etc.).</li>
<li>Tools: callable functions surfaced to the LLM.</li>
<li>Gateway: abstraction over model providers.</li>
</ul>
<p>Proceed through the subsections for practical usage.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="the-basics"><a class="header" href="#the-basics">The Basics</a></h1>
<p>A quick orientation to the core types and flows.</p>
<ul>
<li>Create messages (system, user, assistant)</li>
<li>Configure a completion</li>
<li>Send to a gateway via the broker</li>
</ul>
<pre><pre class="playground"><code class="language-rust">use mojentic::llm::{Broker, CompletionConfig, Message};

#[tokio::main]
async fn main() -&gt; anyhow::Result&lt;()&gt; {
    let broker = Broker::new()?;
    let cfg = CompletionConfig::default();
    let msgs = vec![
        Message::system("You are concise"),
        Message::user("Explain Rust lifetimes in one line"),
    ];
    let out = broker.chat(cfg, msgs).await?;
    println!("{}", out.text());
    Ok(())
}</code></pre></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="message-composers"><a class="header" href="#message-composers">Message Composers</a></h1>
<p>Helpers for building prompts and role-structured messages.</p>
<ul>
<li>System prompts set behavior</li>
<li>User messages provide tasks</li>
<li>Assistant messages chain context</li>
</ul>
<p>Tip: keep messages small and focused; prefer function/tool calls for larger payloads.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="simple-text-generation-1"><a class="header" href="#simple-text-generation-1">Simple Text Generation</a></h1>
<p>The minimal path from prompt to text.</p>
<pre><pre class="playground"><code class="language-rust">use mojentic::llm::{Broker, CompletionConfig, Message};

#[tokio::main]
async fn main() -&gt; anyhow::Result&lt;()&gt; {
    let resp = Broker::new()?
        .chat(CompletionConfig::default(), [Message::user("Haiku about Rust")])
        .await?;
    println!("{}", resp.text());
    Ok(())
}</code></pre></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="structured-output-2"><a class="header" href="#structured-output-2">Structured Output</a></h1>
<p>Generate structured JSON-like responses parsed directly into Rust types.</p>
<p>Design goal: eliminate brittle regex/post-hoc parsing.</p>
<p>(Placeholder: Example using serde + output schema once implemented.)</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="building-tools"><a class="header" href="#building-tools">Building Tools</a></h1>
<h2 id="creating-custom-tools"><a class="header" href="#creating-custom-tools">Creating Custom Tools</a></h2>
<p>Tools extend LLM capabilities by providing functions they can call. All tools must implement the <code>LlmTool</code> trait.</p>
<h3 id="basic-tool-structure"><a class="header" href="#basic-tool-structure">Basic Tool Structure</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use mojentic::prelude::*;
use serde_json::{json, Value};
use std::collections::HashMap;

struct CalculatorTool;

impl LlmTool for CalculatorTool {
    fn run(&amp;self, args: &amp;HashMap&lt;String, Value&gt;) -&gt; mojentic::Result&lt;Value&gt; {
        let operation = args.get("operation")
            .and_then(|v| v.as_str())
            .ok_or_else(|| mojentic::MojenticError::ToolError(
                "Missing operation parameter".to_string()
            ))?;

        let a = args.get("a")
            .and_then(|v| v.as_f64())
            .ok_or_else(|| mojentic::MojenticError::ToolError(
                "Missing 'a' parameter".to_string()
            ))?;

        let b = args.get("b")
            .and_then(|v| v.as_f64())
            .ok_or_else(|| mojentic::MojenticError::ToolError(
                "Missing 'b' parameter".to_string()
            ))?;

        let result = match operation {
            "add" =&gt; a + b,
            "subtract" =&gt; a - b,
            "multiply" =&gt; a * b,
            "divide" =&gt; {
                if b == 0.0 {
                    return Err(mojentic::MojenticError::ToolError(
                        "Division by zero".to_string()
                    ));
                }
                a / b
            }
            _ =&gt; return Err(mojentic::MojenticError::ToolError(
                format!("Unknown operation: {}", operation)
            )),
        };

        Ok(json!({
            "result": result,
            "operation": operation,
            "a": a,
            "b": b
        }))
    }

    fn descriptor(&amp;self) -&gt; ToolDescriptor {
        ToolDescriptor {
            r#type: "function".to_string(),
            function: FunctionDescriptor {
                name: "calculator".to_string(),
                description: "Perform basic arithmetic operations".to_string(),
                parameters: json!({
                    "type": "object",
                    "properties": {
                        "operation": {
                            "type": "string",
                            "enum": ["add", "subtract", "multiply", "divide"],
                            "description": "The operation to perform"
                        },
                        "a": {
                            "type": "number",
                            "description": "First operand"
                        },
                        "b": {
                            "type": "number",
                            "description": "Second operand"
                        }
                    },
                    "required": ["operation", "a", "b"]
                }),
            },
        }
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="using-custom-tools"><a class="header" href="#using-custom-tools">Using Custom Tools</a></h3>
<pre><pre class="playground"><code class="language-rust">#[tokio::main]
async fn main() -&gt; mojentic::Result&lt;()&gt; {
    let gateway = OllamaGateway::new();
    let broker = LlmBroker::new("qwen3:32b", Arc::new(gateway));

    let tools: Vec&lt;Box&lt;dyn LlmTool&gt;&gt; = vec![
        Box::new(CalculatorTool),
    ];

    let messages = vec![
        LlmMessage::user("What is 42 multiplied by 17?")
    ];

    let response = broker.generate(&amp;messages, Some(&amp;tools), None).await?;
    println!("{}", response);

    Ok(())
}</code></pre></pre>
<h2 id="testing-custom-tools"><a class="header" href="#testing-custom-tools">Testing Custom Tools</a></h2>
<p>Create tests for your tools:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_calculator_add() {
        let tool = CalculatorTool;
        let mut args = HashMap::new();
        args.insert("operation".to_string(), json!("add"));
        args.insert("a".to_string(), json!(5.0));
        args.insert("b".to_string(), json!(3.0));

        let result = tool.run(&amp;args).unwrap();
        assert_eq!(result["result"], 8.0);
    }

    #[test]
    fn test_calculator_divide_by_zero() {
        let tool = CalculatorTool;
        let mut args = HashMap::new();
        args.insert("operation".to_string(), json!("divide"));
        args.insert("a".to_string(), json!(10.0));
        args.insert("b".to_string(), json!(0.0));

        assert!(tool.run(&amp;args).is_err());
    }
}
<span class="boring">}</span></code></pre></pre>
<h2 id="best-practices"><a class="header" href="#best-practices">Best Practices</a></h2>
<ol>
<li><strong>Error Handling</strong>: Always use proper error types, never panic in library code</li>
<li><strong>Documentation</strong>: Add rustdoc comments to your tool implementations</li>
<li><strong>Testing</strong>: Write comprehensive tests for your tools</li>
<li><strong>Type Safety</strong>: Validate all input parameters</li>
<li><strong>JSON Schema</strong>: Provide clear, detailed parameter descriptions</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="tool-usage"><a class="header" href="#tool-usage">Tool Usage</a></h1>
<p>Let models invoke your tools to retrieve data or perform actions.</p>
<p>Patterns:</p>
<ul>
<li>Stateless RPC-like tools</li>
<li>Stateful tools with access to context</li>
<li>Safety: validate inputs, timeouts, retries</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="agent-delegation-with-toolwrapper"><a class="header" href="#agent-delegation-with-toolwrapper">Agent Delegation with ToolWrapper</a></h1>
<p>The ToolWrapper enables agent delegation patterns by wrapping an agent (broker + tools + behavior) as a tool that can be used by other agents. This allows you to build hierarchical agent systems where coordinator agents can delegate specialized tasks to expert agents.</p>
<h2 id="overview"><a class="header" href="#overview">Overview</a></h2>
<p>ToolWrapper wraps three core components:</p>
<ul>
<li><strong>Broker</strong>: The LLM interface for the agent</li>
<li><strong>Tools</strong>: The tools available to the agent</li>
<li><strong>Behavior</strong>: The system message defining the agent's personality and capabilities</li>
</ul>
<p>The wrapped agent appears as a standard tool with a single <code>input</code> parameter, making it easy for other agents to delegate work.</p>
<h2 id="basic-usage-1"><a class="header" href="#basic-usage-1">Basic Usage</a></h2>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use mojentic::llm::{LlmBroker, LlmTool, ToolWrapper};
use mojentic::llm::gateways::OllamaGateway;
use std::sync::Arc;

// Create a specialist agent
let gateway = Arc::new(OllamaGateway::default());
let specialist_broker = Arc::new(LlmBroker::new("qwen2.5:7b", gateway.clone()));

let specialist_tools: Vec&lt;Box&lt;dyn LlmTool&gt;&gt; = vec![
    // Add specialist's tools here
];

let specialist_behaviour =
    "You are a specialist in temporal reasoning and date calculations.";

// Wrap the specialist as a tool
let specialist_tool = ToolWrapper::new(
    specialist_broker,
    specialist_tools,
    specialist_behaviour,
    "temporal_specialist",
    "A specialist that handles date and time-related queries."
);

// Use the specialist in a coordinator agent
let coordinator_broker = LlmBroker::new("qwen2.5:14b", gateway);
let coordinator_tools: Vec&lt;Box&lt;dyn LlmTool&gt;&gt; = vec![
    Box::new(specialist_tool)
];

// The coordinator can now delegate to the specialist
<span class="boring">}</span></code></pre></pre>
<h2 id="how-it-works"><a class="header" href="#how-it-works">How It Works</a></h2>
<ol>
<li>
<p><strong>Tool Descriptor</strong>: The ToolWrapper generates a tool descriptor with:</p>
<ul>
<li>Function name (e.g., "temporal_specialist")</li>
<li>Description (what the agent does)</li>
<li>Single <code>input</code> parameter for instructions</li>
</ul>
</li>
<li>
<p><strong>Execution Flow</strong>: When called:</p>
<ul>
<li>Extracts the <code>input</code> parameter</li>
<li>Creates initial messages with the agent's behavior (system message)</li>
<li>Appends the input as a user message</li>
<li>Calls the agent's broker with its tools</li>
<li>Returns the agent's response</li>
</ul>
</li>
<li>
<p><strong>Delegation Pattern</strong>: The coordinator agent sees specialist agents as tools and decides when to delegate based on the task requirements.</p>
</li>
</ol>
<h2 id="example-multi-agent-system"><a class="header" href="#example-multi-agent-system">Example: Multi-Agent System</a></h2>
<p>See <code>examples/broker_as_tool.rs</code> for a complete example demonstrating:</p>
<ul>
<li>Creating specialist agents with specific tools and behaviors</li>
<li>Wrapping specialists as tools</li>
<li>Building a coordinator that delegates appropriately</li>
<li>Testing different query types</li>
</ul>
<pre><code class="language-bash">cargo run --example broker_as_tool
</code></pre>
<h2 id="design-considerations"><a class="header" href="#design-considerations">Design Considerations</a></h2>
<h3 id="agent-ownership"><a class="header" href="#agent-ownership">Agent Ownership</a></h3>
<p>ToolWrapper uses <code>Arc&lt;LlmBroker&gt;</code> to handle shared ownership of the broker. This allows multiple references to the same broker if needed.</p>
<h3 id="async-execution"><a class="header" href="#async-execution">Async Execution</a></h3>
<p>The <code>run</code> method is synchronous (required by the <code>LlmTool</code> trait) but internally handles async operations using <code>tokio::task::block_in_place</code>. This requires tests to use the multi-threaded runtime:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[tokio::test(flavor = "multi_thread")]
async fn test_tool_wrapper() {
    // Test code here
}
<span class="boring">}</span></code></pre></pre>
<h3 id="tool-isolation"><a class="header" href="#tool-isolation">Tool Isolation</a></h3>
<p>Each wrapped agent maintains its own:</p>
<ul>
<li>Tool set</li>
<li>Behavior/personality</li>
<li>Conversation context (per invocation)</li>
</ul>
<p>This ensures clean separation between agents and prevents context bleeding.</p>
<h2 id="common-patterns-1"><a class="header" href="#common-patterns-1">Common Patterns</a></h2>
<h3 id="specialist-agent-pattern"><a class="header" href="#specialist-agent-pattern">Specialist Agent Pattern</a></h3>
<p>Create focused agents with specific expertise:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Data analysis specialist
let data_analyst = ToolWrapper::new(
    analyst_broker,
    vec![Box::new(DataQueryTool), Box::new(VisualizationTool)],
    "You are a data analyst specializing in statistical analysis.",
    "data_analyst",
    "Analyzes data and provides statistical insights."
);

// Writing specialist
let writer = ToolWrapper::new(
    writer_broker,
    vec![Box::new(GrammarCheckTool), Box::new(StyleGuideTool)],
    "You are a professional writer and editor.",
    "writer",
    "Improves and edits written content."
);
<span class="boring">}</span></code></pre></pre>
<h3 id="coordinator-pattern"><a class="header" href="#coordinator-pattern">Coordinator Pattern</a></h3>
<p>Build a coordinator that orchestrates multiple specialists:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let coordinator = LlmBroker::new("qwen2.5:32b", gateway);
let tools: Vec&lt;Box&lt;dyn LlmTool&gt;&gt; = vec![
    Box::new(data_analyst),
    Box::new(writer),
    Box::new(researcher),
];

// Coordinator decides which specialist to use based on the task
<span class="boring">}</span></code></pre></pre>
<h3 id="hierarchical-agents"><a class="header" href="#hierarchical-agents">Hierarchical Agents</a></h3>
<p>Create multi-level hierarchies:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Level 1: Base specialists
let math_specialist = ToolWrapper::new(...);
let physics_specialist = ToolWrapper::new(...);

// Level 2: Domain coordinator
let science_coordinator = ToolWrapper::new(
    coordinator_broker,
    vec![Box::new(math_specialist), Box::new(physics_specialist)],
    "You coordinate math and physics specialists.",
    "science_coordinator",
    "Handles scientific queries."
);

// Level 3: Top-level coordinator
let main_coordinator = LlmBroker::new("qwen2.5:32b", gateway);
let main_tools: Vec&lt;Box&lt;dyn LlmTool&gt;&gt; = vec![
    Box::new(science_coordinator),
    // Other domain coordinators...
];
<span class="boring">}</span></code></pre></pre>
<h2 id="best-practices-1"><a class="header" href="#best-practices-1">Best Practices</a></h2>
<ol>
<li><strong>Clear Specialization</strong>: Give each agent a well-defined area of expertise</li>
<li><strong>Descriptive Names</strong>: Use clear, descriptive names for tools (e.g., "temporal_specialist", not "agent1")</li>
<li><strong>Comprehensive Descriptions</strong>: Write detailed descriptions so coordinators understand when to delegate</li>
<li><strong>Model Selection</strong>: Use appropriate model sizes for each role:
<ul>
<li>Specialists: Smaller, faster models (7B-14B)</li>
<li>Coordinators: Larger, more capable models (32B+)</li>
</ul>
</li>
<li><strong>Tool Composition</strong>: Specialists should have tools relevant to their domain</li>
<li><strong>Testing</strong>: Test both individual specialists and the full delegation chain</li>
</ol>
<h2 id="limitations"><a class="header" href="#limitations">Limitations</a></h2>
<ul>
<li><strong>Synchronous Interface</strong>: The <code>LlmTool</code> trait requires synchronous <code>run</code> methods, though async operations happen internally</li>
<li><strong>No Streaming</strong>: Tool calls don't support streaming responses (limitation of current tool architecture)</li>
<li><strong>Context</strong>: Each tool invocation is independent; no automatic context sharing between calls</li>
<li><strong>Error Handling</strong>: Tool errors propagate up to the coordinator; implement appropriate error handling</li>
</ul>
<h2 id="see-also"><a class="header" href="#see-also">See Also</a></h2>
<ul>
<li><a href="core/tool_usage.html">Tool Usage</a> - General tool usage patterns</li>
<li><a href="core/building_tools.html">Building Tools</a> - Creating custom tools</li>
<li><a href="core/chat_sessions_with_tools.html">Chat Sessions with Tools</a> - Using tools in chat sessions</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="chat-sessions"><a class="header" href="#chat-sessions">Chat Sessions</a></h1>
<p><code>ChatSession</code> provides a high-level abstraction for managing multi-turn conversations with LLMs. It automatically handles conversation history, context window management, and token counting.</p>
<h2 id="key-features"><a class="header" href="#key-features">Key Features</a></h2>
<ul>
<li><strong>Automatic History Management</strong>: Maintains conversation history automatically</li>
<li><strong>Context Window Trimming</strong>: Removes oldest messages when token limit is exceeded</li>
<li><strong>System Prompt Preservation</strong>: Always keeps the system prompt (index 0) intact</li>
<li><strong>Token Counting</strong>: Uses <code>TokenizerGateway</code> for accurate token tracking</li>
<li><strong>Builder Pattern</strong>: Flexible configuration with sensible defaults</li>
</ul>
<h2 id="basic-usage-2"><a class="header" href="#basic-usage-2">Basic Usage</a></h2>
<p>The simplest way to create a chat session:</p>
<pre><pre class="playground"><code class="language-rust">use mojentic::llm::{ChatSession, LlmBroker};
use mojentic::llm::gateways::OllamaGateway;
use std::sync::Arc;

#[tokio::main]
async fn main() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {
    // Create broker
    let gateway = Arc::new(OllamaGateway::default());
    let broker = LlmBroker::new("qwen3:32b", gateway);

    // Create session with defaults
    let mut session = ChatSession::new(broker);

    // Send messages
    let response = session.send("What is Rust?").await?;
    println!("{}", response);

    // Continue the conversation
    let response = session.send("Tell me more about ownership.").await?;
    println!("{}", response);

    Ok(())
}</code></pre></pre>
<h2 id="custom-configuration"><a class="header" href="#custom-configuration">Custom Configuration</a></h2>
<p>Use the builder pattern for custom configuration:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use mojentic::llm::ChatSession;

let session = ChatSession::builder(broker)
    .system_prompt("You are a helpful coding assistant specializing in Rust.")
    .temperature(0.7)
    .max_context(16384)  // 16k tokens
    .build();
<span class="boring">}</span></code></pre></pre>
<h3 id="configuration-options"><a class="header" href="#configuration-options">Configuration Options</a></h3>
<ul>
<li><strong><code>system_prompt</code></strong>: The system message (default: "You are a helpful assistant.")</li>
<li><strong><code>temperature</code></strong>: Sampling temperature (default: 1.0)</li>
<li><strong><code>max_context</code></strong>: Maximum tokens in context window (default: 32768)</li>
<li><strong><code>tokenizer_gateway</code></strong>: Custom tokenizer (default: cl100k_base)</li>
<li><strong><code>tools</code></strong>: Tools available to the LLM (default: None)</li>
</ul>
<h2 id="context-window-management"><a class="header" href="#context-window-management">Context Window Management</a></h2>
<p>When the total token count exceeds <code>max_context</code>, <code>ChatSession</code> automatically removes the oldest messages:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let mut session = ChatSession::builder(broker)
    .max_context(2048)  // Small context window
    .build();

// Add many messages
for i in 0..100 {
    session.send(&amp;format!("Message {}", i)).await?;
}

// Only recent messages are kept
println!("Messages in history: {}", session.messages().len());
println!("Total tokens: {}", session.total_tokens());
<span class="boring">}</span></code></pre></pre>
<p><strong>Important</strong>: The system prompt (index 0) is always preserved, even when trimming.</p>
<h2 id="message-history"><a class="header" href="#message-history">Message History</a></h2>
<p>Access the conversation history:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Get all messages
for msg in session.messages() {
    println!("{:?}: {}", msg.role(), msg.content().unwrap_or(""));
}

// Check token usage
println!("Total tokens: {}", session.total_tokens());
<span class="boring">}</span></code></pre></pre>
<h2 id="sizedllmmessage"><a class="header" href="#sizedllmmessage">SizedLlmMessage</a></h2>
<p>Internally, <code>ChatSession</code> uses <code>SizedLlmMessage</code>, which extends <code>LlmMessage</code> with token count metadata:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct SizedLlmMessage {
    pub message: LlmMessage,
    pub token_length: usize,
}
<span class="boring">}</span></code></pre></pre>
<p>This allows efficient context window management without recounting tokens repeatedly.</p>
<h2 id="interactive-example"><a class="header" href="#interactive-example">Interactive Example</a></h2>
<p>See the complete interactive chat example:</p>
<pre><code class="language-bash">cargo run --example chat_session
</code></pre>
<p>This example demonstrates:</p>
<ul>
<li>Reading user input from stdin</li>
<li>Maintaining conversation context</li>
<li>Displaying token usage</li>
<li>Graceful exit handling</li>
</ul>
<h2 id="best-practices-2"><a class="header" href="#best-practices-2">Best Practices</a></h2>
<ol>
<li><strong>Choose appropriate context size</strong>: Balance between conversation length and performance</li>
<li><strong>Monitor token usage</strong>: Use <code>session.total_tokens()</code> to track usage</li>
<li><strong>Set clear system prompts</strong>: Guide the LLM's behavior from the start</li>
<li><strong>Adjust temperature</strong>: Lower for deterministic responses, higher for creativity</li>
<li><strong>Handle errors gracefully</strong>: Network and API errors can occur</li>
</ol>
<h2 id="error-handling-1"><a class="header" href="#error-handling-1">Error Handling</a></h2>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>match session.send("Hello").await {
    Ok(response) =&gt; println!("Response: {}", response),
    Err(e) =&gt; eprintln!("Error: {}", e),
}
<span class="boring">}</span></code></pre></pre>
<p>Common errors:</p>
<ul>
<li>Network failures connecting to Ollama</li>
<li>Model not available</li>
<li>Context window exceeded before trimming</li>
<li>Tool execution failures (when using tools)</li>
</ul>
<h2 id="next-steps-1"><a class="header" href="#next-steps-1">Next Steps</a></h2>
<ul>
<li>Learn about <a href="core/chat_sessions_with_tools.html">Chat Sessions with Tools</a></li>
<li>Explore <a href="core/tool_usage.html">Tool Usage</a></li>
<li>Read about <a href="core/building_tools.html">Building Tools</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="chat-sessions-with-tools"><a class="header" href="#chat-sessions-with-tools">Chat Sessions with Tools</a></h1>
<p>Combine conversational context with tool calling for grounded, actionable responses. <code>ChatSession</code> seamlessly integrates with Mojentic's tool system to enable function calling within conversations.</p>
<h2 id="overview-1"><a class="header" href="#overview-1">Overview</a></h2>
<p>When tools are provided to a <code>ChatSession</code>, the LLM can call them during the conversation. The broker handles:</p>
<ol>
<li>Detecting tool call requests</li>
<li>Executing the appropriate tools</li>
<li>Adding tool results to the conversation</li>
<li>Continuing the conversation with the LLM</li>
</ol>
<p>This happens transparently - you just call <code>send()</code> as normal.</p>
<h2 id="basic-tool-integration"><a class="header" href="#basic-tool-integration">Basic Tool Integration</a></h2>
<p>Add tools when building the session:</p>
<pre><pre class="playground"><code class="language-rust">use mojentic::llm::{ChatSession, LlmBroker};
use mojentic::llm::gateways::OllamaGateway;
use mojentic::llm::tools::simple_date_tool::SimpleDateTool;
use std::sync::Arc;

#[tokio::main]
async fn main() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {
    let gateway = Arc::new(OllamaGateway::default());
    let broker = LlmBroker::new("qwen3:32b", gateway);

    // Create tools
    let tools: Vec&lt;Box&lt;dyn mojentic::llm::LlmTool&gt;&gt; = vec![
        Box::new(SimpleDateTool),
    ];

    // Build session with tools
    let mut session = ChatSession::builder(broker)
        .system_prompt(
            "You are a helpful assistant. When users ask about dates, \
             use the resolve_date tool to convert relative dates to absolute dates."
        )
        .tools(tools)
        .build();

    // The LLM will automatically use the tool when needed
    let response = session.send("What is tomorrow's date?").await?;
    println!("{}", response);

    Ok(())
}</code></pre></pre>
<h2 id="how-it-works-1"><a class="header" href="#how-it-works-1">How It Works</a></h2>
<ol>
<li><strong>User sends a message</strong>: <code>session.send("What is tomorrow's date?")</code></li>
<li><strong>LLM recognizes need for tool</strong>: Generates a tool call request</li>
<li><strong>Broker executes tool</strong>: Calls <code>SimpleDateTool</code> with appropriate arguments</li>
<li><strong>Tool result added to history</strong>: As a tool response message</li>
<li><strong>LLM generates final response</strong>: Using the tool's output</li>
<li><strong>Response returned to user</strong>: "Tomorrow's date is 2025-11-14"</li>
</ol>
<p>All of this happens in a single <code>send()</code> call.</p>
<h2 id="multiple-tools"><a class="header" href="#multiple-tools">Multiple Tools</a></h2>
<p>Provide multiple tools for different capabilities:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use mojentic::llm::tools::simple_date_tool::SimpleDateTool;
// Import other tools...

let tools: Vec&lt;Box&lt;dyn mojentic::llm::LlmTool&gt;&gt; = vec![
    Box::new(SimpleDateTool),
    Box::new(CalculatorTool),
    Box::new(WeatherTool),
];

let mut session = ChatSession::builder(broker)
    .system_prompt(
        "You are a helpful assistant with access to date resolution, \
         calculation, and weather information tools. Use them when appropriate."
    )
    .tools(tools)
    .build();
<span class="boring">}</span></code></pre></pre>
<p>The LLM will choose which tool to use based on the user's question.</p>
<h2 id="tool-call-history"><a class="header" href="#tool-call-history">Tool Call History</a></h2>
<p>Tool calls and results are part of the conversation history:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// After a tool call
for msg in session.messages() {
    match msg.role() {
        MessageRole::User =&gt; println!("User: {}", msg.content().unwrap_or("")),
        MessageRole::Assistant =&gt; {
            if let Some(tool_calls) = &amp;msg.message.tool_calls {
                println!("Assistant called tool: {:?}", tool_calls);
            } else {
                println!("Assistant: {}", msg.content().unwrap_or(""));
            }
        }
        MessageRole::Tool =&gt; println!("Tool result: {}", msg.content().unwrap_or("")),
        _ =&gt; {}
    }
}
<span class="boring">}</span></code></pre></pre>
<h2 id="context-management-with-tools"><a class="header" href="#context-management-with-tools">Context Management with Tools</a></h2>
<p>Tool calls and results consume tokens. The session manages this automatically:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let mut session = ChatSession::builder(broker)
    .max_context(8192)
    .tools(tools)
    .build();

// Even with tool calls, context trimming works
session.send("What's tomorrow?").await?;  // May trigger tool call
session.send("And the day after?").await?;  // May trigger another tool call

// Old messages (including old tool calls) are trimmed when needed
println!("Total tokens: {}", session.total_tokens());
<span class="boring">}</span></code></pre></pre>
<h2 id="creating-custom-tools-1"><a class="header" href="#creating-custom-tools-1">Creating Custom Tools</a></h2>
<p>Build your own tools for specific needs:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use mojentic::llm::tools::{LlmTool, ToolDescriptor, FunctionDescriptor};
use serde_json::{json, Value};
use std::collections::HashMap;

struct WeatherTool;

impl LlmTool for WeatherTool {
    fn run(&amp;self, args: &amp;HashMap&lt;String, Value&gt;) -&gt; mojentic::error::Result&lt;Value&gt; {
        let city = args.get("city")
            .and_then(|v| v.as_str())
            .ok_or_else(|| mojentic::error::MojenticError::ToolError(
                "Missing city argument".to_string()
            ))?;

        // In reality, you'd call a weather API here
        Ok(json!({
            "city": city,
            "temperature": "72°F",
            "conditions": "Sunny"
        }))
    }

    fn descriptor(&amp;self) -&gt; ToolDescriptor {
        ToolDescriptor {
            r#type: "function".to_string(),
            function: FunctionDescriptor {
                name: "get_weather".to_string(),
                description: "Get current weather for a city".to_string(),
                parameters: json!({
                    "type": "object",
                    "properties": {
                        "city": {
                            "type": "string",
                            "description": "The city name"
                        }
                    },
                    "required": ["city"]
                }),
            },
        }
    }
}
<span class="boring">}</span></code></pre></pre>
<p>Then use it:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let tools: Vec&lt;Box&lt;dyn mojentic::llm::LlmTool&gt;&gt; = vec![
    Box::new(WeatherTool),
];

let mut session = ChatSession::builder(broker)
    .system_prompt("You can check weather using the get_weather tool.")
    .tools(tools)
    .build();

let response = session.send("What's the weather in San Francisco?").await?;
<span class="boring">}</span></code></pre></pre>
<h2 id="interactive-example-1"><a class="header" href="#interactive-example-1">Interactive Example</a></h2>
<p>Run the complete example:</p>
<pre><code class="language-bash">cargo run --example chat_session_with_tool
</code></pre>
<p>This demonstrates:</p>
<ul>
<li>Tool integration in conversation</li>
<li>Automatic tool invocation</li>
<li>Natural multi-turn dialogue with tool results</li>
</ul>
<h2 id="best-practices-3"><a class="header" href="#best-practices-3">Best Practices</a></h2>
<ol>
<li><strong>Clear tool descriptions</strong>: Help the LLM understand when to use each tool</li>
<li><strong>Descriptive system prompts</strong>: Explain what tools are available</li>
<li><strong>Error handling in tools</strong>: Return meaningful errors for invalid inputs</li>
<li><strong>Tool result formatting</strong>: Return structured JSON that's easy for the LLM to interpret</li>
<li><strong>Monitor token usage</strong>: Tools can add significant tokens to the conversation</li>
</ol>
<h2 id="debugging-tool-calls"><a class="header" href="#debugging-tool-calls">Debugging Tool Calls</a></h2>
<p>Enable tracing to see tool execution:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Initialize tracing
tracing_subscriber::fmt::init();

// Tool calls will be logged
let response = session.send("What's tomorrow?").await?;
<span class="boring">}</span></code></pre></pre>
<p>You'll see logs like:</p>
<pre><code>INFO Tool calls requested: 1
INFO Executing tool: resolve_date
</code></pre>
<h2 id="limitations-1"><a class="header" href="#limitations-1">Limitations</a></h2>
<ul>
<li>Tool calls count toward the context window</li>
<li>Very long tool results may need truncation</li>
<li>Not all models support tool calling equally well</li>
<li>Tool execution is synchronous (async tools not yet supported)</li>
</ul>
<h2 id="next-steps-2"><a class="header" href="#next-steps-2">Next Steps</a></h2>
<ul>
<li>Learn about <a href="core/building_tools.html">Building Tools</a></li>
<li>Explore <a href="core/tool_usage.html">Tool Usage</a> patterns</li>
<li>Read about <a href="core/simple_text_generation.html">Simple Text Generation</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="image-analysis"><a class="header" href="#image-analysis">Image Analysis</a></h1>
<p>Vision-capable models can analyze images by passing image file paths along with text prompts. The framework automatically handles reading image files and encoding them as Base64 for transmission to the LLM.</p>
<h2 id="basic-usage-3"><a class="header" href="#basic-usage-3">Basic Usage</a></h2>
<p>Attach images to a message using the <code>with_images()</code> method:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use mojentic::prelude::*;

let message = LlmMessage::user("Describe this image")
    .with_images(vec!["/path/to/image.jpg".to_string()]);
<span class="boring">}</span></code></pre></pre>
<p>You can attach multiple images to a single message:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let message = LlmMessage::user("Compare these images")
    .with_images(vec![
        "/path/to/image1.jpg".to_string(),
        "/path/to/image2.jpg".to_string(),
    ]);
<span class="boring">}</span></code></pre></pre>
<h2 id="complete-example"><a class="header" href="#complete-example">Complete Example</a></h2>
<pre><pre class="playground"><code class="language-rust">use mojentic::prelude::*;
use std::sync::Arc;

#[tokio::main]
async fn main() -&gt; Result&lt;()&gt; {
    // Create gateway and broker with a vision-capable model
    let gateway = OllamaGateway::new();
    let broker = LlmBroker::new("llava:latest", Arc::new(gateway), None);

    // Create a message with image
    let message = LlmMessage::user("What's in this image?")
        .with_images(vec!["path/to/image.jpg".to_string()]);

    // Generate response
    let response = broker.generate(&amp;[message], None, None, None).await?;
    println!("{}", response);

    Ok(())
}</code></pre></pre>
<h2 id="vision-capable-models"><a class="header" href="#vision-capable-models">Vision-Capable Models</a></h2>
<p>Common vision-capable models available through Ollama:</p>
<ul>
<li><code>llava:latest</code> - General-purpose vision model</li>
<li><code>bakllava:latest</code> - BakLLaVA vision model</li>
<li><code>qwen3-vl:30b</code> - Qwen3 vision-language model</li>
<li><code>gemma3:27b</code> - Gemma 3 with vision support</li>
</ul>
<p>Pull a model before using:</p>
<pre><code class="language-bash">ollama pull llava:latest
</code></pre>
<h2 id="how-it-works-2"><a class="header" href="#how-it-works-2">How It Works</a></h2>
<p>When you attach images to a message:</p>
<ol>
<li><strong>File Reading</strong>: The gateway reads the image file from the specified path</li>
<li><strong>Base64 Encoding</strong>: The image bytes are encoded as Base64 using the <code>base64</code> crate</li>
<li><strong>API Transmission</strong>: The encoded image is included in the <code>images</code> field of the Ollama API request</li>
<li><strong>Model Processing</strong>: The vision-capable model analyzes both the text prompt and image(s)</li>
</ol>
<h2 id="error-handling-2"><a class="header" href="#error-handling-2">Error Handling</a></h2>
<p>Image processing can fail if:</p>
<ul>
<li>The image file doesn't exist or isn't readable</li>
<li>The file path is invalid</li>
<li>The model doesn't support vision</li>
</ul>
<p>Always handle errors when working with images:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>match broker.generate(&amp;[message], None, None, None).await {
    Ok(response) =&gt; println!("Response: {}", response),
    Err(e) =&gt; eprintln!("Error analyzing image: {}", e),
}
<span class="boring">}</span></code></pre></pre>
<h2 id="supported-image-formats"><a class="header" href="#supported-image-formats">Supported Image Formats</a></h2>
<p>The framework reads raw image bytes and passes them to the model. Supported formats depend on the specific model being used. Most vision models support common formats like JPEG and PNG.</p>
<h2 id="see-also-1"><a class="header" href="#see-also-1">See Also</a></h2>
<ul>
<li><a href="core/../examples/README.html">Examples</a> - See <code>image_analysis.rs</code> for a working example</li>
<li><a href="core/messages.html">LlmMessage API</a> - Full message construction API</li>
<li><a href="core/../gateways/ollama.html">Ollama Gateway</a> - Gateway-specific details</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="layer-2---agents"><a class="header" href="#layer-2---agents">Layer 2 - Agents</a></h1>
<p>Agents are the smallest unit of computation in Mojentic. They process events and emit new events, composing into larger systems.</p>
<p>This section mirrors the Python project’s agent layer but uses Rust’s async and type system.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="asynchronous-capabilities"><a class="header" href="#asynchronous-capabilities">Asynchronous Capabilities</a></h1>
<p>Mojentic (Rust) embraces async end-to-end.</p>
<ul>
<li>Concurrency for overlapping LLM calls</li>
<li>Channels/streams for event propagation</li>
<li>Timeouts and cancellation for robustness</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="iterative-problem-solver"><a class="header" href="#iterative-problem-solver">Iterative Problem Solver</a></h1>
<p>The <code>IterativeProblemSolver</code> is an agent that iteratively attempts to solve problems using available tools. It employs a chat-based approach and continues working until it succeeds, fails explicitly, or reaches the maximum number of iterations.</p>
<h2 id="overview-2"><a class="header" href="#overview-2">Overview</a></h2>
<p>The Iterative Problem Solver follows a simple but powerful pattern:</p>
<ol>
<li><strong>Plan</strong> - Analyze the problem and identify what needs to be done</li>
<li><strong>Act</strong> - Execute actions using available tools</li>
<li><strong>Observe</strong> - Review the results</li>
<li><strong>Refine</strong> - Adjust the approach based on observations</li>
<li><strong>Terminate</strong> - Stop when the goal is met or the iteration budget is exhausted</li>
</ol>
<h2 id="key-features-1"><a class="header" href="#key-features-1">Key Features</a></h2>
<ul>
<li><strong>Tool Integration</strong>: Seamlessly integrates with any <code>LlmTool</code> implementations</li>
<li><strong>Automatic Termination</strong>: Stops when the LLM responds with "DONE" or "FAIL"</li>
<li><strong>Iteration Control</strong>: Configurable maximum iteration count prevents infinite loops</li>
<li><strong>Chat-Based Context</strong>: Maintains conversation history for context-aware problem solving</li>
<li><strong>Summary Generation</strong>: Provides a clean summary of the final result</li>
</ul>
<h2 id="usage"><a class="header" href="#usage">Usage</a></h2>
<h3 id="basic-example"><a class="header" href="#basic-example">Basic Example</a></h3>
<pre><pre class="playground"><code class="language-rust">use mojentic::agents::IterativeProblemSolver;
use mojentic::llm::{LlmBroker, LlmTool};
use mojentic::llm::gateways::OllamaGateway;
use mojentic::llm::tools::simple_date_tool::SimpleDateTool;
use std::sync::Arc;

#[tokio::main]
async fn main() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {
    // Initialize the LLM broker
    let gateway = Arc::new(OllamaGateway::default());
    let broker = LlmBroker::new("qwen3:32b", gateway, None);

    // Define available tools
    let tools: Vec&lt;Box&lt;dyn LlmTool&gt;&gt; = vec![
        Box::new(SimpleDateTool),
    ];

    // Create the solver
    let mut solver = IterativeProblemSolver::builder(broker)
        .tools(tools)
        .max_iterations(5)
        .build();

    // Solve a problem
    let result = solver.solve("What's the date next Friday?").await?;
    println!("Result: {}", result);

    Ok(())
}</code></pre></pre>
<h3 id="custom-system-prompt"><a class="header" href="#custom-system-prompt">Custom System Prompt</a></h3>
<p>You can customize the system prompt to guide the solver's behavior:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let mut solver = IterativeProblemSolver::builder(broker)
    .tools(tools)
    .max_iterations(10)
    .system_prompt(
        "You are a specialized data analysis assistant. \
         Break down complex queries into clear steps and use tools methodically."
    )
    .build();
<span class="boring">}</span></code></pre></pre>
<h3 id="with-multiple-tools"><a class="header" href="#with-multiple-tools">With Multiple Tools</a></h3>
<p>The solver works best when given appropriate tools for the problem domain:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use mojentic::llm::tools::ask_user_tool::AskUserTool;
use mojentic::llm::tools::simple_date_tool::SimpleDateTool;

let tools: Vec&lt;Box&lt;dyn LlmTool&gt;&gt; = vec![
    Box::new(AskUserTool::new()),
    Box::new(SimpleDateTool),
];

let mut solver = IterativeProblemSolver::builder(broker)
    .tools(tools)
    .max_iterations(5)
    .build();
<span class="boring">}</span></code></pre></pre>
<h2 id="how-it-works-3"><a class="header" href="#how-it-works-3">How It Works</a></h2>
<h3 id="step-by-step-process"><a class="header" href="#step-by-step-process">Step-by-Step Process</a></h3>
<ol>
<li><strong>Initialization</strong>: The solver creates a <code>ChatSession</code> with the provided system prompt and tools</li>
<li><strong>Iteration Loop</strong>: For each iteration:
<ul>
<li>Sends the problem description with instructions to use tools</li>
<li>Checks the response for "DONE" (success) or "FAIL" (failure)</li>
<li>Continues if neither keyword is present and iterations remain</li>
</ul>
</li>
<li><strong>Summary</strong>: After termination, requests a concise summary of the result</li>
<li><strong>Return</strong>: Returns the summary as the final result</li>
</ol>
<h3 id="termination-conditions"><a class="header" href="#termination-conditions">Termination Conditions</a></h3>
<p>The solver terminates when one of these conditions is met:</p>
<ul>
<li><strong>Success</strong>: The LLM's response contains "DONE" (case-insensitive)</li>
<li><strong>Failure</strong>: The LLM's response contains "FAIL" (case-insensitive)</li>
<li><strong>Exhaustion</strong>: The maximum number of iterations is reached</li>
</ul>
<h3 id="logging-1"><a class="header" href="#logging-1">Logging</a></h3>
<p>The solver uses the <code>tracing</code> crate to log important events:</p>
<ul>
<li><code>info</code>: Logged when a task completes successfully or fails</li>
<li><code>warn</code>: Logged when maximum iterations are reached</li>
</ul>
<h2 id="configuration-options-1"><a class="header" href="#configuration-options-1">Configuration Options</a></h2>
<h3 id="builder-pattern"><a class="header" href="#builder-pattern">Builder Pattern</a></h3>
<p>The <code>IterativeProblemSolver</code> uses the builder pattern for configuration:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>IterativeProblemSolver::builder(broker)
    .tools(tools)              // Set available tools
    .max_iterations(10)        // Set max iterations (default: 3)
    .system_prompt("...")      // Set custom system prompt
    .build()
<span class="boring">}</span></code></pre></pre>
<h3 id="default-values"><a class="header" href="#default-values">Default Values</a></h3>
<ul>
<li><strong>max_iterations</strong>: 3</li>
<li><strong>system_prompt</strong>:
<blockquote>
<p>"You are a problem-solving assistant that can solve complex problems step by step.
You analyze problems, break them down into smaller parts, and solve them systematically.
If you cannot solve a problem completely in one step, you make progress and identify what to do next."</p>
</blockquote>
</li>
</ul>
<h2 id="best-practices-4"><a class="header" href="#best-practices-4">Best Practices</a></h2>
<h3 id="1-choose-appropriate-tools"><a class="header" href="#1-choose-appropriate-tools">1. Choose Appropriate Tools</a></h3>
<p>Select tools that are relevant to your problem domain:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// For date/time problems
let tools = vec![Box::new(SimpleDateTool)];

// For user interaction
let tools = vec![Box::new(AskUserTool::new())];

// For data analysis
let tools = vec![
    Box::new(CalculatorTool),
    Box::new(DataRetrievalTool),
];
<span class="boring">}</span></code></pre></pre>
<h3 id="2-set-reasonable-iteration-limits"><a class="header" href="#2-set-reasonable-iteration-limits">2. Set Reasonable Iteration Limits</a></h3>
<p>Balance between giving the solver enough attempts and preventing excessive computation:</p>
<ul>
<li>Simple queries: 3-5 iterations</li>
<li>Complex analyses: 10-15 iterations</li>
<li>Open-ended exploration: 20+ iterations</li>
</ul>
<h3 id="3-provide-context-in-problem-description"><a class="header" href="#3-provide-context-in-problem-description">3. Provide Context in Problem Description</a></h3>
<p>The more context you provide, the better the solver can work:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Less effective
solver.solve("Analyze the data").await?;

// More effective
solver.solve(
    "Analyze the sales data from Q1 2024. \
     Focus on trends in the technology sector. \
     Provide insights on growth patterns."
).await?;
<span class="boring">}</span></code></pre></pre>
<h3 id="4-monitor-logs"><a class="header" href="#4-monitor-logs">4. Monitor Logs</a></h3>
<p>Enable tracing to understand solver behavior:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>tracing_subscriber::fmt()
    .with_max_level(tracing::Level::INFO)
    .init();
<span class="boring">}</span></code></pre></pre>
<h2 id="common-patterns-2"><a class="header" href="#common-patterns-2">Common Patterns</a></h2>
<h3 id="retry-logic"><a class="header" href="#retry-logic">Retry Logic</a></h3>
<p>For operations that might fail transiently:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let mut attempts = 0;
let max_attempts = 3;

let result = loop {
    attempts += 1;
    match solver.solve(problem).await {
        Ok(result) if !result.contains("FAIL") =&gt; break result,
        Ok(_) if attempts &lt; max_attempts =&gt; continue,
        Ok(result) =&gt; break result,
        Err(e) =&gt; return Err(e),
    }
};
<span class="boring">}</span></code></pre></pre>
<h3 id="multi-stage-problems"><a class="header" href="#multi-stage-problems">Multi-Stage Problems</a></h3>
<p>For problems that require multiple phases:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Phase 1: Data gathering
let mut solver = IterativeProblemSolver::builder(broker.clone())
    .tools(data_tools)
    .max_iterations(5)
    .build();
let data = solver.solve("Gather all relevant data").await?;

// Phase 2: Analysis
let mut solver = IterativeProblemSolver::builder(broker)
    .tools(analysis_tools)
    .max_iterations(10)
    .build();
let analysis = solver.solve(&amp;format!("Analyze: {}", data)).await?;
<span class="boring">}</span></code></pre></pre>
<h2 id="examples"><a class="header" href="#examples">Examples</a></h2>
<p>See the complete examples at:</p>
<ul>
<li><code>examples/iterative_solver.rs</code> - Basic usage with date and user interaction tools</li>
<li><code>examples/solver_chat_session.rs</code> - Interactive chat session with solver delegation pattern</li>
</ul>
<h2 id="error-handling-3"><a class="header" href="#error-handling-3">Error Handling</a></h2>
<p>The solver returns <code>Result&lt;String, MojenticError&gt;</code>:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>match solver.solve(problem).await {
    Ok(result) =&gt; println!("Solution: {}", result),
    Err(MojenticError::GatewayError(msg)) =&gt; {
        eprintln!("Gateway error: {}", msg);
    }
    Err(MojenticError::ToolError(msg)) =&gt; {
        eprintln!("Tool error: {}", msg);
    }
    Err(e) =&gt; {
        eprintln!("Unexpected error: {}", e);
    }
}
<span class="boring">}</span></code></pre></pre>
<h2 id="advanced-solver-as-a-tool"><a class="header" href="#advanced-solver-as-a-tool">Advanced: Solver as a Tool</a></h2>
<p>The <code>IterativeProblemSolver</code> can be wrapped as a tool and used within a <code>ChatSession</code>, enabling powerful delegation patterns where a chat assistant can offload complex problems to a specialized solver agent.</p>
<h3 id="creating-a-solver-tool"><a class="header" href="#creating-a-solver-tool">Creating a Solver Tool</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use mojentic::llm::tools::{FunctionDescriptor, LlmTool, ToolDescriptor};
use mojentic::agents::IterativeProblemSolver;
use serde_json::{json, Value};
use std::collections::HashMap;
use std::sync::Arc;

struct IterativeProblemSolverTool {
    broker: Arc&lt;LlmBroker&gt;,
    tools: Vec&lt;Box&lt;dyn LlmTool&gt;&gt;,
}

impl IterativeProblemSolverTool {
    fn new(broker: Arc&lt;LlmBroker&gt;, tools: Vec&lt;Box&lt;dyn LlmTool&gt;&gt;) -&gt; Self {
        Self { broker, tools }
    }
}

impl Clone for IterativeProblemSolverTool {
    fn clone(&amp;self) -&gt; Self {
        Self {
            broker: self.broker.clone(),
            tools: self.tools.iter().map(|t| t.clone_box()).collect(),
        }
    }
}

impl LlmTool for IterativeProblemSolverTool {
    fn run(&amp;self, args: &amp;HashMap&lt;String, Value&gt;) -&gt; mojentic::error::Result&lt;Value&gt; {
        let problem_to_solve = args
            .get("problem_to_solve")
            .and_then(|v| v.as_str())
            .ok_or_else(|| {
                mojentic::error::MojenticError::ToolError(
                    "Missing required argument: problem_to_solve".to_string(),
                )
            })?;

        let solver_tools: Vec&lt;Box&lt;dyn LlmTool&gt;&gt; =
            self.tools.iter().map(|t| t.clone_box()).collect();

        let runtime = tokio::runtime::Handle::current();
        let broker_clone = (*self.broker).clone();

        let result = runtime.block_on(async move {
            let mut solver = IterativeProblemSolver::builder(broker_clone)
                .tools(solver_tools)
                .max_iterations(5)
                .build();

            solver.solve(problem_to_solve).await
        })?;

        Ok(json!({"solution": result}))
    }

    fn descriptor(&amp;self) -&gt; ToolDescriptor {
        ToolDescriptor {
            r#type: "function".to_string(),
            function: FunctionDescriptor {
                name: "iterative_problem_solver".to_string(),
                description: "Iteratively solve a complex multi-step problem using available tools.".to_string(),
                parameters: json!({
                    "type": "object",
                    "properties": {
                        "problem_to_solve": {
                            "type": "string",
                            "description": "The problem or request to be solved."
                        }
                    },
                    "required": ["problem_to_solve"],
                    "additionalProperties": false
                }),
            },
        }
    }

    fn clone_box(&amp;self) -&gt; Box&lt;dyn LlmTool&gt; {
        Box::new(self.clone())
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="using-the-solver-tool-in-a-chat-session"><a class="header" href="#using-the-solver-tool-in-a-chat-session">Using the Solver Tool in a Chat Session</a></h3>
<pre><pre class="playground"><code class="language-rust">use mojentic::llm::{ChatSession, LlmBroker};
use mojentic::llm::tools::simple_date_tool::SimpleDateTool;

#[tokio::main]
async fn main() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {
    let gateway = Arc::new(OllamaGateway::default());
    let broker = Arc::new(LlmBroker::new("qwq", gateway, None));

    // Create the solver tool with SimpleDateTool as the inner tool
    let solver_tools: Vec&lt;Box&lt;dyn LlmTool&gt;&gt; = vec![Box::new(SimpleDateTool)];
    let solver_tool = IterativeProblemSolverTool::new(broker.clone(), solver_tools);

    // Create chat session with the solver tool
    let mut session = ChatSession::builder((*broker).clone())
        .system_prompt(
            "You are a helpful assistant with access to an iterative problem solver. \
             When faced with complex multi-step problems or questions that require \
             reasoning and tool usage, use the iterative_problem_solver tool."
        )
        .tools(vec![Box::new(solver_tool)])
        .build();

    // Interactive loop
    loop {
        let mut query = String::new();
        print!("Query: ");
        io::stdout().flush()?;
        io::stdin().read_line(&amp;mut query)?;

        if query.trim().is_empty() {
            break;
        }

        let response = session.send(query.trim()).await?;
        println!("{}\n", response);
    }

    Ok(())
}</code></pre></pre>
<h3 id="benefits-of-this-pattern"><a class="header" href="#benefits-of-this-pattern">Benefits of This Pattern</a></h3>
<ol>
<li><strong>Delegation</strong>: The chat assistant can offload complex problems to a specialized solver</li>
<li><strong>Composability</strong>: Mix solver capabilities with other tools in the same session</li>
<li><strong>Context Preservation</strong>: The chat session maintains conversation history</li>
<li><strong>Flexible Interaction</strong>: Users can ask simple questions directly or complex problems that trigger the solver</li>
</ol>
<p>See the complete example at:</p>
<ul>
<li><code>examples/solver_chat_session.rs</code> - Interactive chat session with solver delegation</li>
</ul>
<h2 id="limitations-2"><a class="header" href="#limitations-2">Limitations</a></h2>
<ul>
<li><strong>LLM Dependency</strong>: Quality of results depends on the underlying LLM's capabilities</li>
<li><strong>Tool Design</strong>: Effectiveness relies on well-designed tools with clear descriptions</li>
<li><strong>Token Limits</strong>: Long iterations may hit context window limits</li>
<li><strong>Cost</strong>: Multiple LLM calls per problem can increase API costs</li>
</ul>
<h2 id="see-also-2"><a class="header" href="#see-also-2">See Also</a></h2>
<ul>
<li><a href="agents/../core/chat_sessions.html">Chat Sessions</a> - Understanding the underlying chat mechanism</li>
<li><a href="agents/../core/building_tools.html">Building Tools</a> - Creating custom tools for the solver</li>
<li><a href="agents/sra.html">Simple Recursive Agent</a> - Alternative problem-solving pattern</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="simple-recursive-agent"><a class="header" href="#simple-recursive-agent">Simple Recursive Agent</a></h1>
<p>The <code>SimpleRecursiveAgent</code> provides an event-driven approach to iterative problem-solving with LLMs. It automatically handles retries, tool execution, and state management while emitting events at each step for monitoring.</p>
<h2 id="overview-3"><a class="header" href="#overview-3">Overview</a></h2>
<p>The SimpleRecursiveAgent:</p>
<ul>
<li>Solves problems through iterative refinement</li>
<li>Emits events at each step for monitoring and debugging</li>
<li>Handles tool execution automatically via ChatSession</li>
<li>Stops when it finds a solution, fails, or reaches max iterations</li>
<li>Provides timeout protection (300 seconds default)</li>
</ul>
<h2 id="basic-usage-4"><a class="header" href="#basic-usage-4">Basic Usage</a></h2>
<pre><pre class="playground"><code class="language-rust">use mojentic::agents::SimpleRecursiveAgent;
use mojentic::llm::{LlmBroker, LlmMessage};
use mojentic::llm::gateways::OllamaGateway;

#[tokio::main]
async fn main() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {
    let gateway = OllamaGateway::default();
    let broker = Arc::new(LlmBroker::new("qwen3:32b", gateway));

    // Create agent with 5 max iterations
    let agent = SimpleRecursiveAgent::new(broker, Vec::new(), Some(5), None);

    // Solve a problem
    let solution = agent.solve("What is the capital of France?").await?;
    println!("{}", solution);

    Ok(())
}</code></pre></pre>
<h2 id="with-tools"><a class="header" href="#with-tools">With Tools</a></h2>
<p>The agent can use tools to gather information or perform actions:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use mojentic::llm::tools::simple_date_tool::SimpleDateTool;
use std::sync::Arc;

let tools: Vec&lt;Box&lt;dyn LlmTool&gt;&gt; = vec![Box::new(SimpleDateTool)];

let agent = SimpleRecursiveAgent::new(
    broker,
    tools,
    Some(5),  // Max iterations
    None      // Use default system prompt
);

let solution = agent.solve("What's the date next Friday?").await?;
<span class="boring">}</span></code></pre></pre>
<h2 id="event-monitoring"><a class="header" href="#event-monitoring">Event Monitoring</a></h2>
<p>Subscribe to events to monitor the problem-solving process:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use mojentic::agents::{SimpleRecursiveAgent, RecursiveAgentEvent};

let agent = SimpleRecursiveAgent::new(broker, vec![], Some(5), None);

// Subscribe to events before solving
let subscription = agent.subscribe({
    let agent_clone = agent.clone();
    move |event| {
        match event {
            RecursiveAgentEvent::GoalSubmitted { goal } =&gt; {
                println!("Goal submitted: {}", goal);
            }
            RecursiveAgentEvent::IterationCompleted { iteration, response, .. } =&gt; {
                println!("Iteration {}: {}", iteration, response);
            }
            RecursiveAgentEvent::GoalAchieved { solution, iterations } =&gt; {
                println!("Success after {} iterations!", iterations);
            }
            RecursiveAgentEvent::GoalFailed { reason } =&gt; {
                eprintln!("Failed: {}", reason);
            }
            RecursiveAgentEvent::Timeout =&gt; {
                eprintln!("Timeout after 300 seconds");
            }
        }
    }
});

let solution = agent.solve("Complex problem").await?;

// Unsubscribe when done
agent.unsubscribe(subscription);
<span class="boring">}</span></code></pre></pre>
<h2 id="custom-system-prompt-1"><a class="header" href="#custom-system-prompt-1">Custom System Prompt</a></h2>
<p>Customize the agent's behavior with a custom system prompt:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let custom_prompt =
    "You are a concise assistant that provides brief, factual answers. \
     Always respond in exactly one sentence.";

let agent = SimpleRecursiveAgent::new(
    broker,
    vec![],
    Some(5),
    Some(custom_prompt.to_string())
);
<span class="boring">}</span></code></pre></pre>
<h2 id="goal-state"><a class="header" href="#goal-state">Goal State</a></h2>
<p>The state object that tracks the problem-solving process:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct GoalState {
    pub goal: String,
    pub iteration: usize,
    pub max_iterations: usize,
    pub solution: Option&lt;String&gt;,
    pub is_complete: bool,
}
<span class="boring">}</span></code></pre></pre>
<h2 id="event-types"><a class="header" href="#event-types">Event Types</a></h2>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub enum RecursiveAgentEvent {
    GoalSubmitted { goal: String },
    IterationCompleted {
        iteration: usize,
        response: String,
        state: GoalState,
    },
    GoalAchieved {
        solution: String,
        iterations: usize,
    },
    GoalFailed { reason: String },
    Timeout,
}
<span class="boring">}</span></code></pre></pre>
<h2 id="completion-criteria"><a class="header" href="#completion-criteria">Completion Criteria</a></h2>
<p>The agent stops iterating when:</p>
<ol>
<li><strong>Success</strong>: The LLM response contains "DONE" (case-insensitive)</li>
<li><strong>Failure</strong>: The LLM response contains "FAIL" (case-insensitive)</li>
<li><strong>Max Iterations</strong>: The iteration count reaches <code>max_iterations</code></li>
<li><strong>Timeout</strong>: 300 seconds have elapsed</li>
</ol>
<p>When stopped at max iterations, the last response is returned as the best available solution.</p>
<h2 id="api-reference"><a class="header" href="#api-reference">API Reference</a></h2>
<h3 id="constructor"><a class="header" href="#constructor">Constructor</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>impl SimpleRecursiveAgent {
    pub fn new(
        llm: Arc&lt;LlmBroker&gt;,
        available_tools: Vec&lt;Box&lt;dyn LlmTool&gt;&gt;,
        max_iterations: Option&lt;usize&gt;,
        system_prompt: Option&lt;String&gt;,
    ) -&gt; Self
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Parameters:</strong></p>
<ul>
<li><code>llm</code>: The LLM broker to use for generating responses</li>
<li><code>available_tools</code>: Vector of tools the agent can use</li>
<li><code>max_iterations</code>: Maximum number of iterations (default: 5)</li>
<li><code>system_prompt</code>: Custom system prompt (default: problem-solving assistant prompt)</li>
</ul>
<h3 id="methods"><a class="header" href="#methods">Methods</a></h3>
<h4 id="solveself-problem-str---result"><a class="header" href="#solveself-problem-str---result">solve(&amp;self, problem: &amp;str) -&gt; Result<String></a></h4>
<p>Solve a problem asynchronously.</p>
<p><strong>Parameters:</strong></p>
<ul>
<li><code>problem</code>: The problem to solve</li>
</ul>
<p><strong>Returns:</strong> <code>Result&lt;String&gt;</code> containing the solution</p>
<p><strong>Errors:</strong> Returns error if the solution cannot be found within 300 seconds</p>
<h4 id="subscribeself-callback-f---usize"><a class="header" href="#subscribeself-callback-f---usize">subscribe<F>(&amp;self, callback: F) -&gt; usize</a></h4>
<p>Subscribe to agent events.</p>
<p><strong>Parameters:</strong></p>
<ul>
<li><code>callback</code>: Function to call when events occur</li>
</ul>
<p><strong>Returns:</strong> Subscription ID for unsubscribing</p>
<h4 id="unsubscribeself-id-usize"><a class="header" href="#unsubscribeself-id-usize">unsubscribe(&amp;self, id: usize)</a></h4>
<p>Unsubscribe from events using subscription ID.</p>
<h2 id="best-practices-5"><a class="header" href="#best-practices-5">Best Practices</a></h2>
<ol>
<li>
<p><strong>Use Arc for sharing</strong>: The agent uses <code>Arc&lt;LlmBroker&gt;</code> for thread-safe sharing:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let broker = Arc::new(LlmBroker::new("model", gateway));
let agent = SimpleRecursiveAgent::new(broker, vec![], Some(5), None);
<span class="boring">}</span></code></pre></pre>
</li>
<li>
<p><strong>Set appropriate max iterations</strong>: Balance between thoroughness and performance:</p>
<ul>
<li>Simple queries: 3-5 iterations</li>
<li>Complex problems: 10-20 iterations</li>
</ul>
</li>
<li>
<p><strong>Use event monitoring for debugging</strong>: Subscribe to events during development to understand the agent's reasoning process</p>
</li>
<li>
<p><strong>Clean up subscriptions</strong>: Always unsubscribe when done to prevent memory leaks</p>
</li>
<li>
<p><strong>Provide clear problem statements</strong>: The more specific your problem description, the better the agent can solve it</p>
</li>
</ol>
<h2 id="example-complete-workflow"><a class="header" href="#example-complete-workflow">Example: Complete Workflow</a></h2>
<pre><pre class="playground"><code class="language-rust">use mojentic::agents::{SimpleRecursiveAgent, RecursiveAgentEvent};
use mojentic::llm::{LlmBroker, LlmMessage};
use mojentic::llm::gateways::OllamaGateway;
use mojentic::llm::tools::simple_date_tool::SimpleDateTool;
use std::sync::Arc;

#[tokio::main]
async fn main() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {
    let gateway = OllamaGateway::default();
    let broker = Arc::new(LlmBroker::new("qwen3:32b", gateway));

    let tools: Vec&lt;Box&lt;dyn LlmTool&gt;&gt; = vec![Box::new(SimpleDateTool)];

    let agent = SimpleRecursiveAgent::new(broker, tools, Some(5), None);

    // Log progress
    let sub = agent.subscribe(|event| {
        match event {
            RecursiveAgentEvent::IterationCompleted { iteration, .. } =&gt; {
                println!("Iteration {}/5", iteration);
            }
            RecursiveAgentEvent::GoalAchieved { iterations, .. } =&gt; {
                println!("✓ Solved in {} iterations", iterations);
            }
            _ =&gt; {}
        }
    });

    let solution = agent
        .solve("What's the date two Fridays from now?")
        .await?;

    println!("\nSolution: {}", solution);

    agent.unsubscribe(sub);

    Ok(())
}</code></pre></pre>
<h2 id="concurrent-problem-solving"><a class="header" href="#concurrent-problem-solving">Concurrent Problem Solving</a></h2>
<p>The agent is thread-safe and can solve multiple problems concurrently:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use tokio::task;

let agent = Arc::new(SimpleRecursiveAgent::new(broker, vec![], Some(3), None));

let problems = vec![
    "What is the Pythagorean theorem?",
    "Explain recursion in programming.",
];

let handles: Vec&lt;_&gt; = problems
    .into_iter()
    .map(|problem| {
        let agent = Arc::clone(&amp;agent);
        task::spawn(async move { agent.solve(problem).await })
    })
    .collect();

for (i, handle) in handles.into_iter().enumerate() {
    let solution = handle.await??;
    println!("Problem {}: {}", i + 1, solution);
}
<span class="boring">}</span></code></pre></pre>
<h2 id="comparison-with-iterativeproblemsolver"><a class="header" href="#comparison-with-iterativeproblemsolver">Comparison with IterativeProblemSolver</a></h2>
<p>Both agents solve problems iteratively, but they differ in approach:</p>
<p><strong>SimpleRecursiveAgent:</strong></p>
<ul>
<li>Event-driven architecture</li>
<li>Explicit event types for each stage</li>
<li>Manual event subscription for monitoring</li>
<li>300-second hard timeout</li>
<li>Best for: Custom event handling, complex workflows, debugging</li>
</ul>
<p><strong>IterativeProblemSolver:</strong></p>
<ul>
<li>Simpler API, minimal boilerplate</li>
<li>Direct access to chat history</li>
<li>Best for: Quick prototyping, straightforward tasks</li>
</ul>
<p>Choose <code>SimpleRecursiveAgent</code> when you need fine-grained control and visibility into the problem-solving process. Choose <code>IterativeProblemSolver</code> for simpler use cases.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="working-memory-pattern"><a class="header" href="#working-memory-pattern">Working Memory Pattern</a></h1>
<p>The working memory pattern enables agents to maintain and share context across multiple interactions. This guide shows how to use <code>SharedWorkingMemory</code> and build memory-aware agents in Rust.</p>
<h2 id="overview-4"><a class="header" href="#overview-4">Overview</a></h2>
<p>Working memory provides:</p>
<ul>
<li><strong>Shared Context</strong>: Multiple agents can read from and write to the same memory</li>
<li><strong>Continuous Learning</strong>: Agents automatically learn and remember new information</li>
<li><strong>State Persistence</strong>: Knowledge is maintained across interactions</li>
<li><strong>Thread Safety</strong>: Safe concurrent access using <code>Arc&lt;Mutex&lt;T&gt;&gt;</code></li>
</ul>
<h2 id="quick-start"><a class="header" href="#quick-start">Quick Start</a></h2>
<h3 id="basic-usage-5"><a class="header" href="#basic-usage-5">Basic Usage</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use mojentic::context::SharedWorkingMemory;
use serde_json::json;

// Create memory with initial data
let memory = SharedWorkingMemory::new(json!({
    "User": {
        "name": "Alice",
        "age": 30
    }
}));

// Retrieve current state
let current = memory.get_working_memory();

// Update memory (deep merge)
memory.merge_to_working_memory(&amp;json!({
    "User": {
        "city": "NYC",
        "preferences": {
            "theme": "dark"
        }
    }
}));

// Result: {"User": {"name": "Alice", "age": 30, "city": "NYC", "preferences": {...}}}
<span class="boring">}</span></code></pre></pre>
<h3 id="memory-aware-agent-pattern"><a class="header" href="#memory-aware-agent-pattern">Memory-Aware Agent Pattern</a></h3>
<pre><pre class="playground"><code class="language-rust">use mojentic::llm::{LlmBroker, LlmMessage};
use mojentic::llm::gateways::OllamaGateway;
use mojentic::context::SharedWorkingMemory;
use serde_json::json;

#[tokio::main]
async fn main() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {
    // Initialize
    let gateway = OllamaGateway::default();
    let broker = LlmBroker::new("qwen2.5:7b", gateway);
    let memory = SharedWorkingMemory::new(json!({
        "User": {"name": "Alice"}
    }));

    // Create prompt with memory context
    let memory_context = memory.get_working_memory();
    let prompt = format!(
        "This is what you remember: {}\n\nRemember anything new you learn.\n\nUser: I love pizza",
        serde_json::to_string_pretty(&amp;memory_context)?
    );

    // Generate response with schema that includes memory field
    let schema = json!({
        "type": "object",
        "required": ["answer"],
        "properties": {
            "answer": {"type": "string"},
            "memory": {
                "type": "object",
                "description": "Add anything new you learned here."
            }
        }
    });

    let response = broker.generate(
        vec![LlmMessage::user(&amp;prompt)],
        Some(schema),
        None,
        None,
        None,
        None
    ).await?;

    // Parse response and update memory
    let response_json: serde_json::Value = serde_json::from_str(&amp;response.content)?;
    if let Some(learned) = response_json.get("memory") {
        memory.merge_to_working_memory(learned);
    }

    Ok(())
}</code></pre></pre>
<h2 id="core-concepts"><a class="header" href="#core-concepts">Core Concepts</a></h2>
<h3 id="sharedworkingmemory"><a class="header" href="#sharedworkingmemory">SharedWorkingMemory</a></h3>
<p>A thread-safe, mutable key-value store that agents use to share context:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct SharedWorkingMemory {
    memory: Arc&lt;Mutex&lt;HashMap&lt;String, Value&gt;&gt;&gt;,
}

impl SharedWorkingMemory {
    pub fn new(initial: Value) -&gt; Self
    pub fn get_working_memory(&amp;self) -&gt; Value
    pub fn merge_to_working_memory(&amp;self, updates: &amp;Value)
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Key features:</strong></p>
<ul>
<li><strong>Thread-Safe</strong>: Uses <code>Arc&lt;Mutex&lt;T&gt;&gt;</code> for concurrent access</li>
<li><strong>Deep Merge</strong>: Nested JSON objects are recursively merged</li>
<li><strong>Simple API</strong>: Just 3 methods to learn</li>
</ul>
<h2 id="deep-merge-behavior"><a class="header" href="#deep-merge-behavior">Deep Merge Behavior</a></h2>
<p>Memory updates use deep merge to preserve existing data:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use serde_json::json;

// Initial memory
let memory = SharedWorkingMemory::new(json!({
    "User": {
        "name": "Alice",
        "age": 30,
        "address": {
            "city": "NYC",
            "state": "NY"
        }
    }
}));

// Update with nested data
memory.merge_to_working_memory(&amp;json!({
    "User": {
        "age": 31,
        "address": {
            "zip": "10001"
        }
    }
}));

// Result: All fields preserved, nested objects merged
// {
//   "User": {
//     "name": "Alice",      // Preserved
//     "age": 31,            // Updated
//     "address": {
//       "city": "NYC",      // Preserved
//       "state": "NY",      // Preserved
//       "zip": "10001"      // Added
//     }
//   }
// }
<span class="boring">}</span></code></pre></pre>
<h2 id="building-memory-aware-agents"><a class="header" href="#building-memory-aware-agents">Building Memory-Aware Agents</a></h2>
<p>Here's a complete pattern for memory-aware agents:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use mojentic::llm::{LlmBroker, LlmMessage};
use mojentic::context::SharedWorkingMemory;
use mojentic::agents::{Event, BaseAsyncAgent};
use serde::{Deserialize, Serialize};
use serde_json::json;
use async_trait::async_trait;

#[derive(Serialize, Deserialize)]
struct ResponseModel {
    text: String,
    #[serde(skip_serializing_if = "Option::is_none")]
    memory: Option&lt;serde_json::Value&gt;,
}

struct MemoryAgent {
    broker: Arc&lt;LlmBroker&gt;,
    memory: SharedWorkingMemory,
    behaviour: String,
    instructions: String,
}

impl MemoryAgent {
    fn new(
        broker: Arc&lt;LlmBroker&gt;,
        memory: SharedWorkingMemory,
        behaviour: String,
        instructions: String,
    ) -&gt; Self {
        Self { broker, memory, behaviour, instructions }
    }

    async fn generate_with_memory(
        &amp;self,
        user_input: &amp;str,
    ) -&gt; Result&lt;ResponseModel, Box&lt;dyn std::error::Error&gt;&gt; {
        // Build prompt with memory context
        let memory_context = self.memory.get_working_memory();
        let messages = vec![
            LlmMessage::system(&amp;self.behaviour),
            LlmMessage::user(&amp;format!(
                "This is what you remember:\n{}\n\n{}",
                serde_json::to_string_pretty(&amp;memory_context)?,
                self.instructions
            )),
            LlmMessage::user(user_input),
        ];

        // Schema with memory field
        let schema = json!({
            "type": "object",
            "required": ["text"],
            "properties": {
                "text": {"type": "string"},
                "memory": {
                    "type": "object",
                    "description": "Add new information here."
                }
            }
        });

        // Generate response
        let response = self.broker.generate(
            messages,
            Some(schema),
            None,
            None,
            None,
            None
        ).await?;

        // Parse and update memory
        let result: ResponseModel = serde_json::from_str(&amp;response.content)?;
        if let Some(ref learned) = result.memory {
            self.memory.merge_to_working_memory(learned);
        }

        Ok(result)
    }
}
<span class="boring">}</span></code></pre></pre>
<h2 id="multi-agent-coordination"><a class="header" href="#multi-agent-coordination">Multi-Agent Coordination</a></h2>
<p>Multiple agents can share the same memory instance:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use std::sync::Arc;

// Shared memory
let memory = Arc::new(SharedWorkingMemory::new(json!({
    "context": {}
})));

// Multiple agents
let researcher = MemoryAgent::new(
    Arc::clone(&amp;broker),
    Arc::clone(&amp;memory),
    "You are a research assistant.".to_string(),
    "Research topics thoroughly.".to_string(),
);

let writer = MemoryAgent::new(
    Arc::clone(&amp;broker),
    Arc::clone(&amp;memory),
    "You are a technical writer.".to_string(),
    "Write clear documentation.".to_string(),
);

// Researcher updates memory
let research = researcher
    .generate_with_memory("Research Rust async patterns")
    .await?;

// Writer uses updated memory (already shared)
let article = writer
    .generate_with_memory("Write an article about what was researched")
    .await?;
<span class="boring">}</span></code></pre></pre>
<h2 id="use-cases"><a class="header" href="#use-cases">Use Cases</a></h2>
<h3 id="1-conversational-chatbots"><a class="header" href="#1-conversational-chatbots">1. Conversational Chatbots</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let memory = SharedWorkingMemory::new(json!({
    "conversation_history": [],
    "user_preferences": {}
}));
<span class="boring">}</span></code></pre></pre>
<h3 id="2-workflow-automation"><a class="header" href="#2-workflow-automation">2. Workflow Automation</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let memory = SharedWorkingMemory::new(json!({
    "workflow_state": "started",
    "completed_steps": [],
    "pending_tasks": []
}));
<span class="boring">}</span></code></pre></pre>
<h3 id="3-knowledge-base-building"><a class="header" href="#3-knowledge-base-building">3. Knowledge Base Building</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let memory = SharedWorkingMemory::new(json!({
    "entities": {},
    "relationships": [],
    "facts": []
}));
<span class="boring">}</span></code></pre></pre>
<h2 id="best-practices-6"><a class="header" href="#best-practices-6">Best Practices</a></h2>
<h3 id="1-structure-your-memory"><a class="header" href="#1-structure-your-memory">1. Structure Your Memory</a></h3>
<p>Use clear, hierarchical keys:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>json!({
    "User": {...},
    "Conversation": {...},
    "SystemState": {...}
})
<span class="boring">}</span></code></pre></pre>
<h3 id="2-use-arc-for-sharing"><a class="header" href="#2-use-arc-for-sharing">2. Use Arc for Sharing</a></h3>
<p>Share memory across threads/agents:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let memory = Arc::new(SharedWorkingMemory::new(initial));
let memory_clone = Arc::clone(&amp;memory);
<span class="boring">}</span></code></pre></pre>
<h3 id="3-validate-memory-updates"><a class="header" href="#3-validate-memory-updates">3. Validate Memory Updates</a></h3>
<p>Check memory quality before accepting:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>if let Some(ref learned) = result.memory {
    if is_valid_update(learned) {
        memory.merge_to_working_memory(learned);
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="4-handle-errors-gracefully"><a class="header" href="#4-handle-errors-gracefully">4. Handle Errors Gracefully</a></h3>
<p>Memory operations can fail:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>match agent.generate_with_memory(input).await {
    Ok(response) =&gt; {
        // Process response
    }
    Err(e) =&gt; {
        eprintln!("Failed to generate response: {}", e);
        // Don't update memory on error
    }
}
<span class="boring">}</span></code></pre></pre>
<h2 id="example-application"><a class="header" href="#example-application">Example Application</a></h2>
<p>See the complete working memory example:</p>
<pre><code class="language-bash">cd mojentic-ru
cargo run --example working_memory
</code></pre>
<p>The example demonstrates:</p>
<ul>
<li>Initializing memory with user data</li>
<li>RequestAgent that learns from conversation</li>
<li>Event-driven coordination with AsyncDispatcher</li>
<li>Memory persistence across interactions</li>
</ul>
<h2 id="api-reference-1"><a class="header" href="#api-reference-1">API Reference</a></h2>
<h3 id="sharedworkingmemory-1"><a class="header" href="#sharedworkingmemory-1">SharedWorkingMemory</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>impl SharedWorkingMemory {
    /// Create new memory with initial data
    pub fn new(initial: Value) -&gt; Self

    /// Get current memory snapshot
    pub fn get_working_memory(&amp;self) -&gt; Value

    /// Deep merge updates into memory
    pub fn merge_to_working_memory(&amp;self, updates: &amp;Value)
}
<span class="boring">}</span></code></pre></pre>
<p>See <code>src/context/shared_working_memory.rs</code> for full documentation.</p>
<h2 id="thread-safety"><a class="header" href="#thread-safety">Thread Safety</a></h2>
<p><code>SharedWorkingMemory</code> is thread-safe and can be shared across async tasks:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use tokio::task;

let memory = Arc::new(SharedWorkingMemory::new(initial));

let task1 = {
    let memory = Arc::clone(&amp;memory);
    task::spawn(async move {
        memory.merge_to_working_memory(&amp;updates1);
    })
};

let task2 = {
    let memory = Arc::clone(&amp;memory);
    task::spawn(async move {
        memory.merge_to_working_memory(&amp;updates2);
    })
};

task1.await?;
task2.await?;
<span class="boring">}</span></code></pre></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="observability"><a class="header" href="#observability">Observability</a></h1>
<p>Tracing and introspection help debug agentic flows, measure latency, and optimize tool usage.</p>
<p>Components:</p>
<ul>
<li>Tracer API for spans/events</li>
<li>Instrumented broker calls</li>
<li>Future: metrics export</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="tracer-system"><a class="header" href="#tracer-system">Tracer System</a></h1>
<p>The tracer system provides comprehensive observability into LLM interactions, tool executions, and agent communications. It records events with timestamps and correlation IDs, enabling detailed debugging and monitoring.</p>
<p>Goals:</p>
<ul>
<li>Performance insight</li>
<li>Failure localization</li>
<li>Reproducibility of agent flows</li>
</ul>
<h2 id="architecture"><a class="header" href="#architecture">Architecture</a></h2>
<p>The tracer system consists of several key components:</p>
<h3 id="core-components"><a class="header" href="#core-components">Core Components</a></h3>
<ul>
<li><strong>TracerEvent</strong>: Base trait for all event types with timestamps, correlation IDs, and printable summaries</li>
<li><strong>EventStore</strong>: Thread-safe storage for events with callbacks and filtering capabilities</li>
<li><strong>TracerSystem</strong>: Coordination layer providing convenience methods for recording events</li>
<li><strong>NullTracer</strong>: Null object pattern implementation for when tracing is disabled</li>
</ul>
<h3 id="event-types-1"><a class="header" href="#event-types-1">Event Types</a></h3>
<p>The system supports four main event types:</p>
<ol>
<li><strong>LlmCallTracerEvent</strong>: Records LLM calls with model, messages, temperature, and available tools</li>
<li><strong>LlmResponseTracerEvent</strong>: Records LLM responses with content, tool calls, and duration</li>
<li><strong>ToolCallTracerEvent</strong>: Records tool executions with arguments, results, and duration</li>
<li><strong>AgentInteractionTracerEvent</strong>: Records agent-to-agent communications</li>
</ol>
<h2 id="usage-1"><a class="header" href="#usage-1">Usage</a></h2>
<h3 id="basic-setup"><a class="header" href="#basic-setup">Basic Setup</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use mojentic::tracer::TracerSystem;
use std::sync::Arc;

// Create a tracer system
let tracer = Arc::new(TracerSystem::default());

// Enable/disable tracing
tracer.enable();
tracer.disable();
<span class="boring">}</span></code></pre></pre>
<h3 id="recording-events"><a class="header" href="#recording-events">Recording Events</a></h3>
<h4 id="llm-calls"><a class="header" href="#llm-calls">LLM Calls</a></h4>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use std::collections::HashMap;

tracer.record_llm_call(
    "llama3.2",                    // model
    vec![],                        // messages (simplified)
    0.7,                           // temperature
    None,                          // tools
    "my_broker",                   // source
    "correlation-123"              // correlation_id
);
<span class="boring">}</span></code></pre></pre>
<h4 id="llm-responses"><a class="header" href="#llm-responses">LLM Responses</a></h4>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>tracer.record_llm_response(
    "llama3.2",                    // model
    "Response text",               // content
    None,                          // tool_calls
    Some(150.5),                   // call_duration_ms
    "my_broker",                   // source
    "correlation-123"              // correlation_id
);
<span class="boring">}</span></code></pre></pre>
<h4 id="tool-calls"><a class="header" href="#tool-calls">Tool Calls</a></h4>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use serde_json::json;

let mut arguments = HashMap::new();
arguments.insert("input".to_string(), json!("test data"));

tracer.record_tool_call(
    "date_tool",                   // tool_name
    arguments,                     // arguments
    json!({"result": "2024-01-15"}), // result
    Some("agent1".to_string()),    // caller
    Some(25.0),                    // call_duration_ms
    "tool_executor",               // source
    "correlation-123"              // correlation_id
);
<span class="boring">}</span></code></pre></pre>
<h4 id="agent-interactions"><a class="header" href="#agent-interactions">Agent Interactions</a></h4>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>tracer.record_agent_interaction(
    "agent1",                      // from_agent
    "agent2",                      // to_agent
    "request",                     // event_type
    Some("evt-456".to_string()),   // event_id
    "dispatcher",                  // source
    "correlation-123"              // correlation_id
);
<span class="boring">}</span></code></pre></pre>
<h3 id="querying-events"><a class="header" href="#querying-events">Querying Events</a></h3>
<h4 id="get-all-events"><a class="header" href="#get-all-events">Get All Events</a></h4>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let summaries = tracer.get_event_summaries(None, None, None);
for summary in summaries {
    println!("{}", summary);
}
<span class="boring">}</span></code></pre></pre>
<h4 id="filter-by-time-range"><a class="header" href="#filter-by-time-range">Filter by Time Range</a></h4>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use std::time::{SystemTime, UNIX_EPOCH};

let now = SystemTime::now()
    .duration_since(UNIX_EPOCH)
    .unwrap()
    .as_secs_f64();

let one_hour_ago = now - 3600.0;

let recent_events = tracer.get_event_summaries(
    Some(one_hour_ago),  // start_time
    Some(now),           // end_time
    None                 // filter_func
);
<span class="boring">}</span></code></pre></pre>
<h4 id="filter-by-custom-predicate"><a class="header" href="#filter-by-custom-predicate">Filter by Custom Predicate</a></h4>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Find all events from a specific correlation ID
let correlation_id = "correlation-123";
let related_events = tracer.get_event_summaries(
    None,
    None,
    Some(&amp;|event| event.correlation_id() == correlation_id)
);

// Count tool call events
let tool_call_count = tracer.count_events(
    None,
    None,
    Some(&amp;|event| {
        event.printable_summary().contains("ToolCallTracerEvent")
    })
);
<span class="boring">}</span></code></pre></pre>
<h4 id="get-last-n-events"><a class="header" href="#get-last-n-events">Get Last N Events</a></h4>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Get last 10 events
let last_events = tracer.get_last_n_summaries(10, None);

// Get last 5 LLM-related events
let last_llm_events = tracer.get_last_n_summaries(
    5,
    Some(&amp;|event| {
        let summary = event.printable_summary();
        summary.contains("LlmCallTracerEvent") ||
        summary.contains("LlmResponseTracerEvent")
    })
);
<span class="boring">}</span></code></pre></pre>
<h3 id="managing-events"><a class="header" href="#managing-events">Managing Events</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Get event count
let total_events = tracer.len();
println!("Total events: {}", total_events);

// Check if empty
if tracer.is_empty() {
    println!("No events recorded");
}

// Clear all events
tracer.clear();
<span class="boring">}</span></code></pre></pre>
<h2 id="correlation-ids"><a class="header" href="#correlation-ids">Correlation IDs</a></h2>
<p>Correlation IDs are UUIDs that track related events across the system. They enable you to trace all events related to a single request or operation, creating a complete audit trail.</p>
<h3 id="best-practices-7"><a class="header" href="#best-practices-7">Best Practices</a></h3>
<ol>
<li><strong>Generate Once</strong>: Create a correlation ID at the start of a request</li>
<li><strong>Pass Through</strong>: Copy the correlation ID to all downstream operations</li>
<li><strong>Query Together</strong>: Use correlation IDs to filter related events</li>
</ol>
<p>Example flow:</p>
<pre><code>User Request → Generate correlation_id
  ↓
LLM Call (correlation_id: "abc-123")
  ↓
LLM Response (correlation_id: "abc-123")
  ↓
Tool Call (correlation_id: "abc-123")
  ↓
LLM Call with tool result (correlation_id: "abc-123")
  ↓
Final LLM Response (correlation_id: "abc-123")
</code></pre>
<h2 id="event-store-callbacks"><a class="header" href="#event-store-callbacks">Event Store Callbacks</a></h2>
<p>You can register a callback function that's called whenever an event is stored:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use mojentic::tracer::{EventStore, EventCallback, TracerSystem};
use std::sync::{Arc, atomic::{AtomicUsize, Ordering}};

// Create a counter
let event_count = Arc::new(AtomicUsize::new(0));
let count_clone = Arc::clone(&amp;event_count);

// Create callback
let callback: EventCallback = Arc::new(move |event| {
    count_clone.fetch_add(1, Ordering::SeqCst);
    println!("Event stored: {}", event.printable_summary());
});

// Create event store with callback
let event_store = Arc::new(EventStore::new(Some(callback)));

// Create tracer with custom event store
let tracer = Arc::new(TracerSystem::new(Some(event_store), true));

// Events will trigger the callback
tracer.record_llm_call("llama3.2", vec![], 1.0, None, "test", "corr-1");

println!("Total events: {}", event_count.load(Ordering::SeqCst));
<span class="boring">}</span></code></pre></pre>
<h2 id="null-tracer"><a class="header" href="#null-tracer">Null Tracer</a></h2>
<p>For environments where tracing should be completely disabled without conditional checks:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use mojentic::tracer::NullTracer;
use std::collections::HashMap;
use serde_json::json;

let tracer = NullTracer::new();

// All operations are no-ops
tracer.record_llm_call("model", vec![], 1.0, None, "source", "id");
tracer.record_tool_call("tool", HashMap::new(), json!({}), None, None, "source", "id");

// Queries return empty results
assert!(tracer.is_empty());
assert_eq!(tracer.len(), 0);
assert_eq!(tracer.get_event_summaries(None, None, None).len(), 0);
<span class="boring">}</span></code></pre></pre>
<h2 id="performance-considerations"><a class="header" href="#performance-considerations">Performance Considerations</a></h2>
<ol>
<li><strong>Thread Safety</strong>: EventStore uses Arc&lt;Mutex&lt;&gt;&gt; for thread-safe access</li>
<li><strong>Memory</strong>: Events are stored in memory; clear periodically for long-running processes</li>
<li><strong>Overhead</strong>: Minimal when disabled; consider NullTracer for zero overhead</li>
<li><strong>Callbacks</strong>: Keep callback functions fast to avoid blocking event recording</li>
</ol>
<h2 id="integration-with-llmbroker"><a class="header" href="#integration-with-llmbroker">Integration with LlmBroker</a></h2>
<p>The tracer integrates with <code>LlmBroker</code> to automatically record LLM interactions (implementation in progress):</p>
<pre><code class="language-rust ignore">use mojentic::llm::{LlmBroker, LlmMessage};
use mojentic::llm::gateways::OllamaGateway;
use mojentic::tracer::TracerSystem;
use std::sync::Arc;

// Create tracer
let tracer = Arc::new(TracerSystem::default());

// Create broker with tracer
let gateway = Arc::new(OllamaGateway::default());
let broker = LlmBroker::builder("llama3.2", gateway)
    .with_tracer(tracer.clone())
    .build();

// Broker will automatically record events
let response = broker.generate(
    &amp;[LlmMessage::user("Hello!")],
    None,
    None
).await?;

// Query tracer for events
let events = tracer.get_event_summaries(None, None, None);
for event in events {
    println!("{}", event);
}</code></pre>
<h2 id="testing"><a class="header" href="#testing">Testing</a></h2>
<p>The tracer system includes comprehensive unit tests covering:</p>
<ul>
<li>Event creation and formatting</li>
<li>Event storage and retrieval</li>
<li>Callbacks and filtering</li>
<li>TracerSystem coordination</li>
<li>NullTracer behavior</li>
</ul>
<p>Run tests with:</p>
<pre><code class="language-bash">cargo test tracer
</code></pre>
<h2 id="implementation-status"><a class="header" href="#implementation-status">Implementation Status</a></h2>
<p><strong>✅ Layer 2 Tracer System - Core Complete</strong></p>
<p>Current implementation:</p>
<ul>
<li>✅ TracerEvent trait with 4 event types</li>
<li>✅ EventStore with callbacks and filtering</li>
<li>✅ TracerSystem coordination layer</li>
<li>✅ NullTracer for zero-overhead tracing</li>
<li>✅ 24 comprehensive unit tests (all passing)</li>
<li>✅ Correlation ID support</li>
<li>✅ Documentation complete</li>
</ul>
<p>Pending integration:</p>
<ul>
<li>⏳ LlmBroker integration (add tracer parameter)</li>
<li>⏳ Tool system integration (add tracer parameter)</li>
<li>⏳ Example application (tracer_demo.rs)</li>
</ul>
<h2 id="see-also-3"><a class="header" href="#see-also-3">See Also</a></h2>
<ul>
<li><a href="observability/./README.html">Observability Overview</a></li>
<li><a href="observability/../core/README.html">Error Handling</a></li>
<li><a href="observability/../broker.html">LLM Integration</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="comprehensive-guide"><a class="header" href="#comprehensive-guide">Comprehensive Guide</a></h1>
<p>End-to-end walkthrough of observing a multi-agent run.</p>
<p>(Placeholder for full example once agents crate stabilized.)</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="api-documentation"><a class="header" href="#api-documentation">API Documentation</a></h1>
<p>The full Rust API reference is generated with <code>cargo doc</code> and published under <code>/api/</code> of the documentation site.</p>
<ul>
<li>Visit: /api/</li>
<li>Crate landing page: /api/mojentic/</li>
</ul>
<p>Note: During local development, run:</p>
<pre><code class="language-sh"># Build API docs
cargo doc --no-deps --all-features
# Build the mdBook
mdbook build book
# Preview by serving book/book/ and opening ./api/ in a second tab from target/doc/
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="contributing"><a class="header" href="#contributing">Contributing</a></h1>
<p>We welcome issues and PRs. Please follow Rust style (rustfmt, clippy) and add tests for new behavior.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="extending-mojentic"><a class="header" href="#extending-mojentic">Extending Mojentic</a></h1>
<p>This guide shows how to extend the Mojentic framework with new gateways and custom functionality.</p>
<h2 id="adding-a-new-llm-gateway"><a class="header" href="#adding-a-new-llm-gateway">Adding a New LLM Gateway</a></h2>
<p>To add support for a new LLM provider, implement the <code>LlmGateway</code> trait:</p>
<h3 id="1-create-the-gateway-file"><a class="header" href="#1-create-the-gateway-file">1. Create the Gateway File</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// src/llm/gateways/openai.rs
use crate::error::{MojenticError, Result};
use crate::llm::gateway::{CompletionConfig, LlmGateway};
use crate::llm::models::{LlmGatewayResponse, LlmMessage};
use crate::llm::tools::LlmTool;
use async_trait::async_trait;
use serde_json::Value;

pub struct OpenAiGateway {
    client: reqwest::Client,
    api_key: String,
    base_url: String,
}

impl OpenAiGateway {
    pub fn new(api_key: String) -&gt; Self {
        Self {
            client: reqwest::Client::new(),
            api_key,
            base_url: "https://api.openai.com/v1".to_string(),
        }
    }
}

#[async_trait]
impl LlmGateway for OpenAiGateway {
    async fn complete(
        &amp;self,
        model: &amp;str,
        messages: &amp;[LlmMessage],
        tools: Option&lt;&amp;[Box&lt;dyn LlmTool&gt;]&gt;,
        config: &amp;CompletionConfig,
    ) -&gt; Result&lt;LlmGatewayResponse&gt; {
        // Implement OpenAI completion API call
        todo!()
    }

    async fn complete_json(
        &amp;self,
        model: &amp;str,
        messages: &amp;[LlmMessage],
        schema: Value,
        config: &amp;CompletionConfig,
    ) -&gt; Result&lt;Value&gt; {
        // Implement OpenAI structured output
        todo!()
    }

    async fn get_available_models(&amp;self) -&gt; Result&lt;Vec&lt;String&gt;&gt; {
        // Implement model listing
        todo!()
    }

    async fn calculate_embeddings(
        &amp;self,
        text: &amp;str,
        model: Option&lt;&amp;str&gt;,
    ) -&gt; Result&lt;Vec&lt;f32&gt;&gt; {
        // Implement embeddings API
        todo!()
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="2-export-the-gateway"><a class="header" href="#2-export-the-gateway">2. Export the Gateway</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// src/llm/gateways/mod.rs
pub mod ollama;
pub mod openai;  // Add this line

pub use ollama::{OllamaConfig, OllamaGateway};
pub use openai::OpenAiGateway;  // Add this line
<span class="boring">}</span></code></pre></pre>
<h3 id="3-use-the-gateway"><a class="header" href="#3-use-the-gateway">3. Use the Gateway</a></h3>
<pre><pre class="playground"><code class="language-rust">use mojentic::prelude::*;
use mojentic::llm::gateways::OpenAiGateway;
use std::sync::Arc;

#[tokio::main]
async fn main() -&gt; Result&lt;()&gt; {
    let gateway = OpenAiGateway::new("your-api-key".to_string());
    let broker = LlmBroker::new("gpt-4", Arc::new(gateway));

    // Use as normal
    let messages = vec![LlmMessage::user("Hello!")];
    let response = broker.generate(&amp;messages, None, None).await?;

    Ok(())
}</code></pre></pre>
<h2 id="creating-custom-message-types"><a class="header" href="#creating-custom-message-types">Creating Custom Message Types</a></h2>
<p>You can create helper functions for specific message types:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use mojentic::llm::models::{LlmMessage, MessageRole};

impl LlmMessage {
    /// Create a message with an image
    pub fn user_with_image(content: impl Into&lt;String&gt;, image_path: impl Into&lt;String&gt;) -&gt; Self {
        Self {
            role: MessageRole::User,
            content: Some(content.into()),
            tool_calls: None,
            image_paths: Some(vec![image_path.into()]),
        }
    }

    /// Create a system message with specific formatting
    pub fn system_instruction(instruction: impl Into&lt;String&gt;) -&gt; Self {
        let content = format!("SYSTEM INSTRUCTION: {}", instruction.into());
        Self::system(content)
    }
}
<span class="boring">}</span></code></pre></pre>
<h2 id="implementing-structured-output-types"><a class="header" href="#implementing-structured-output-types">Implementing Structured Output Types</a></h2>
<p>Define your own structured output types with JSON Schema:</p>
<pre><pre class="playground"><code class="language-rust">use serde::{Deserialize, Serialize};
use schemars::JsonSchema;

#[derive(Debug, Serialize, Deserialize, JsonSchema)]
struct ProductReview {
    rating: u8,
    pros: Vec&lt;String&gt;,
    cons: Vec&lt;String&gt;,
    recommendation: bool,
    summary: String,
}

#[tokio::main]
async fn main() -&gt; mojentic::Result&lt;()&gt; {
    let gateway = OllamaGateway::new();
    let broker = LlmBroker::new("qwen3:32b", Arc::new(gateway));

    let messages = vec![
        LlmMessage::user(
            "Review this product: A wireless mouse with RGB lighting, \
             3200 DPI, 6 buttons, and 40-hour battery life."
        )
    ];

    let review: ProductReview = broker.generate_object(&amp;messages, None).await?;

    println!("Rating: {}/5", review.rating);
    println!("Pros: {:?}", review.pros);
    println!("Cons: {:?}", review.cons);
    println!("Recommended: {}", review.recommendation);

    Ok(())
}</code></pre></pre>
<h2 id="advanced-custom-gateway-configuration"><a class="header" href="#advanced-custom-gateway-configuration">Advanced: Custom Gateway Configuration</a></h2>
<p>Create configuration structs for your gateways:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[derive(Debug, Clone)]
pub struct CustomGatewayConfig {
    pub endpoint: String,
    pub timeout: std::time::Duration,
    pub retry_attempts: u32,
    pub custom_headers: HashMap&lt;String, String&gt;,
}

impl Default for CustomGatewayConfig {
    fn default() -&gt; Self {
        Self {
            endpoint: std::env::var("CUSTOM_ENDPOINT")
                .unwrap_or_else(|_| "https://api.example.com".to_string()),
            timeout: std::time::Duration::from_secs(30),
            retry_attempts: 3,
            custom_headers: HashMap::new(),
        }
    }
}

pub struct CustomGateway {
    client: reqwest::Client,
    config: CustomGatewayConfig,
}

impl CustomGateway {
    pub fn new() -&gt; Self {
        Self::with_config(CustomGatewayConfig::default())
    }

    pub fn with_config(config: CustomGatewayConfig) -&gt; Self {
        let client = reqwest::Client::builder()
            .timeout(config.timeout)
            .build()
            .unwrap();

        Self { client, config }
    }
}
<span class="boring">}</span></code></pre></pre>
<h2 id="testing-your-extensions"><a class="header" href="#testing-your-extensions">Testing Your Extensions</a></h2>
<p>Create tests for your gateways and tools:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[cfg(test)]
mod tests {
    use super::*;

    #[tokio::test]
    async fn test_gateway_models() {
        let gateway = OllamaGateway::new();
        let models = gateway.get_available_models().await.unwrap();
        assert!(!models.is_empty());
    }
}
<span class="boring">}</span></code></pre></pre>
<h2 id="best-practices-8"><a class="header" href="#best-practices-8">Best Practices</a></h2>
<ol>
<li><strong>Error Handling</strong>: Always use proper error types, never panic in library code</li>
<li><strong>Async</strong>: All I/O operations should be async</li>
<li><strong>Documentation</strong>: Add rustdoc comments to all public APIs</li>
<li><strong>Testing</strong>: Write tests for your implementations</li>
<li><strong>Configuration</strong>: Support environment variables for API keys and endpoints</li>
<li><strong>Type Safety</strong>: Leverage Rust's type system for compile-time guarantees</li>
</ol>
<h2 id="need-help"><a class="header" href="#need-help">Need Help?</a></h2>
<ul>
<li>Check the existing implementations in <code>src/llm/gateways/ollama.rs</code></li>
<li>Review the tool example in <code>examples/tool_usage.rs</code></li>
<li>See the <a href="contributing/../core/building_tools.html">Building Tools</a> guide for tool development</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="testing-1"><a class="header" href="#testing-1">Testing</a></h1>
<ul>
<li>Unit tests: <code>cargo test --all-features</code></li>
<li>Lints: <code>cargo clippy -- -D warnings</code></li>
<li>Formatting: <code>cargo fmt -- --check</code></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="user-personas"><a class="header" href="#user-personas">User Personas</a></h1>
<ul>
<li>Framework user: wants ready-to-run examples</li>
<li>Library integrator: needs stable APIs and performance</li>
<li>Contributor: deep dives into internals and design</li>
</ul>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js"></script>
        <script src="mark.min.js"></script>
        <script src="searcher.js"></script>

        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->

        <script>
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>


    </div>
    </body>
</html>
